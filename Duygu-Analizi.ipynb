{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install tensorflow\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git\n",
    "%pip install transformers datasets\n",
    "%pip install torch\n",
    "%pip install tensorflow\n",
    "%pip install pyautogui\n",
    "%pip install snowballstemmer\n",
    "%pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import pyautogui\n",
    "import tweepy\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "from snowballstemmer import TurkishStemmer as snowTurkish\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as st\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream ,OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from trnlp import TrnlpWord\n",
    "from trnlp import *\n",
    "from transformers import AutoModelForSequenceClassification , AutoTokenizer, TextClassificationPipeline, pipeline, AutoModelWithLMHead,AutoModel\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    \n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"Bağlantı Yapıldı!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler Kayıt Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "            \n",
    "            DataSet['Kullanıcı_Adı'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Degistirilmis'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Ceviri']=[tweet.full_text for tweet in tweets]            \n",
    "            #DataSet['Kullanıcı_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            DataSet.to_csv('Datasets/tw.csv',index=False)\n",
    "            print (\"Veriler Kayıt Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,arananTweet):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulanıyor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=arananTweet,result_type=\"new\", lang='tr',tweet_mode=\"extended\")\n",
    "            results=[]\n",
    "            for item in cursor.items(150):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulandı!\")\n",
    "            DataSet = getTweets.toDataFrame(results)            \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            Dataset=pd.read_csv('Datasets/tw.csv',encoding=\"utf-8\")                   \n",
    "            getTweets.TweetClear(DataSet)#Tweet Temizlemek için\n",
    "            getTweets.SentenceNormalization(DataSet)#Türkçe hatalı yazılmış kelimeleri düzeltmek için\n",
    "            getTweets.TweetTranslate(DataSet)#Tweetleri İngilizceye çevirmek için\n",
    "            getTweets.EngWordSeparation(DataSet)#İngilizce kelime Stopwords \n",
    "            getTweets.TrWordSeparation(DataSet)#Türkçe kelime Stopwords\n",
    "            getTweets.ToxicSentence(DataSet)#İngilizce Argo cümle bulmak için             \n",
    "            getTweets.TrStemmed(DataSet)#Türkçe Kelime köklerini almak için\n",
    "            getTweets.EngStemmed(DataSet)#İngilizce Kelime köklerini almak için\n",
    "            getTweets.TextblobSentiment(DataSet)#Textblob Duygu Analizi (Kelime pozitif negatif olarak sorguluyor)           \n",
    "            DataSet.to_csv('Datasets/tw.csv',index=False)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Duygu Analizi Yapılıyor...\")            \n",
    "            data[\"Tür\"] = data[\"Tweet_Ayrı_Eng\"].apply(getTweets.getPolarity)\n",
    "            data[\"Yoğunluk\"] = data[\"Tweet_Ayrı_Eng\"].apply(getTweets.getSubjectivity)\n",
    "            data.to_csv('Datasets/textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi Yapıldı!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            data[\"Tweet_Clear\"]=data[\"Tweet\"]\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|$[a-zA-ZğüşöçıİĞÜŞÖÇ]+|@[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ_]+|[,.^_$*%-;鶯!?’\"“”:=|…#]+|\\n+|amp[\\s]+|gt;')\n",
    "            for index in range(len(data[\"Tweet_Clear\"])):        \n",
    "                data[\"Tweet_Clear\"][index]=pattern.sub(' ', data[\"Tweet_Clear\"][index])    \n",
    "                data[\"Tweet_Clear\"][index]=data[\"Tweet_Clear\"][index].lower()              \n",
    "            data[\"Tweet_Clear\"]=data[\"Tweet_Clear\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))  \n",
    "                              \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Çeviriliyor...\")\n",
    "            data[\"Tweet_Eng\"]=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Eng\"])):\n",
    "                Ceviri_Blob=TextBlob(data[\"Tweet_Eng\"][index])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                data[\"Tweet_Eng\"][index]=Eng_Blob  \n",
    "                data[\"Tweet_Eng\"][index]=data[\"Tweet_Eng\"][index].lower()\n",
    "            print (\"Tweetler Çevirildi!\")                      \n",
    "            return data      \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Cümle Normalizasyonu Yapılıyor...\")\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "            for index in range(len(data[\"Tweet_Clear\"])):                \n",
    "                data[\"Tweet_Clear\"][index]= normalizer.normalize(data[\"Tweet_Clear\"][index].lower())\n",
    "            print(\"Cümle Normalizasyonu Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TrWordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapılıyor...\")\n",
    "            data[\"Tweet_Ayrı_TR\"]=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Ayrı_TR\"])):\n",
    "                data[\"Tweet_Ayrı_TR\"][index] = simple_token(data[\"Tweet_Ayrı_TR\"][index], sw=stopwords)\n",
    "            data[\"Tweet_Ayrı_TR\"] = data[\"Tweet_Ayrı_TR\"].str.join(\" \")\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))  \n",
    "                  \n",
    "    def EngWordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapılıyor...\")\n",
    "            additional  = ['sq','gt','mk','aq','qq','amk']\n",
    "            stop = st.words('english')\n",
    "            \n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Eng\"].str.split()           \n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].apply(lambda x: [item for item in x if item not in additional])\n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].str.join(\" \")\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TrStemmed(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            data[\"TR_Kök\"] = data[\"Tweet_Ayrı_TR\"]\n",
    "            # Kök Bulma\n",
    "            stemmer = snowTurkish()\n",
    "\n",
    "            stemmed_lists = []\n",
    "            for index in tqdm(data[\"TR_Kök\"].index):\n",
    "                mini_l = []\n",
    "                for text in data.loc[index][\"TR_Kök\"].split(\" \"):\n",
    "                    mini_l.append(stemmer.stemWord(text))\n",
    "\n",
    "                big_text=\" \"\n",
    "                for char in mini_l:\n",
    "                    big_text = big_text + \" \" + char\n",
    "                stemmed_lists.append(big_text)\n",
    "    \n",
    "            DataSet['TR_Kök'] = stemmed_lists\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def EngStemmed(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            data[\"Eng_Kök\"] = data[\"Tweet_Ayrı_Eng\"]\n",
    "            # Kök Bulma\n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "            stemmed_lists = []\n",
    "            for index in tqdm(data[\"Eng_Kök\"].index):\n",
    "                mini_l = []\n",
    "                for text in data.loc[index][\"Eng_Kök\"].split(\" \"):\n",
    "                    mini_l.append(stemmer.stem(text))\n",
    "\n",
    "                big_text=\" \"\n",
    "                for char in mini_l:\n",
    "                    big_text = big_text + \" \" + char\n",
    "                stemmed_lists.append(big_text)\n",
    "    \n",
    "            DataSet['Eng_Kök'] = stemmed_lists\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def ToxicSentence(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Toxic Cümle Analizi Yapılıyor...\")\n",
    "            model_path =\"martin-ha/toxic-comment-model\"\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "            data[\"Tweet_Toxic\"]=data[\"Tweet_Eng\"]\n",
    "            temp=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Toxic\"])):\n",
    "                sonuc=pipeline(str(data[\"Tweet_Toxic\"][index]))\n",
    "                if sonuc[0]['label']==\"toxic\":\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic\"\n",
    "                else:\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic değil\"    \n",
    "            print(\"Toxic Cümle Analizi Yapıldı!\")\n",
    "            return data  \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"%75 - %100\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"%50 - %75\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"%25 - %50\"\n",
    "            else:\n",
    "                return \"0 - 25%\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"Nötr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "while True:\n",
    "    aramaTuru=pyautogui.confirm('Ne Tür Bir Arama Yapacaksınız?',title='Arama Türü', buttons=['Hastag','Kelime','Kullanıcı'])\n",
    "    if aramaTuru==\"Hastag\":\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"#\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kelime':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kullanıcı':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"@\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        secim=pyautogui.confirm(text='Hatalı Seçim Yaptınız Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"Kapat\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veri=pd.read_csv('Datasets/textblob_eng_duygu.csv')\n",
    "Veri.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc420cb",
   "metadata": {},
   "source": [
    "### BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veri1=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','Tür','Yoğunluk','Tweet_Clear'])\n",
    "# Eğitilmiş Ağırlıkların Yüklenmesi\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
    "\n",
    "sentiment_analysis_pipeline= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "sentiment_result=[]\n",
    "for text in tqdm(Veri1[\"Tweet_Clear\"]):\n",
    "    result = sentiment_analysis_pipeline(text)[0]\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([Veri1,sentiment_result],axis=1)\n",
    "tweets.label = tweets.label.map({'positive':'Pozitif','negative':'Negatif'})\n",
    "tweets.to_csv(\"Datasets/Bert_Tr_Sonuc.csv\",index=False)\n",
    "\n",
    "Veri2=pd.read_csv(\"Datasets/Bert_Tr_Sonuc.csv\")\n",
    "Veri2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Veri2.label.map({\"Pozitif\":'Olumlu',\"Negatif\":'Olumsuz'}).value_counts().index.values\n",
    "sizes = Veri2.label.value_counts().values\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90,explode=None)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17cea40a",
   "metadata": {},
   "source": [
    "### ELECTRA (BERT MODEL TABANLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91311fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Veri1=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','Tür','Yoğunluk','Tweet_Clear'])\n",
    "# Eğitilmiş Ağırlıkların Yüklenmesi\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "\n",
    "sentiment_analysis_pipeline= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "sentiment_result=[]\n",
    "for text in tqdm(Veri1[\"Tweet_Clear\"]):\n",
    "    result = sentiment_analysis_pipeline(text)\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([Veri1,sentiment_result],axis=1)\n",
    "tweets.label = tweets.label.map({'positive':'Pozitif','negative':'Negatif'})\n",
    "tweets.to_csv(\"Datasets/Bert_Tr_Sonuc2.csv\",index=False)\n",
    "\n",
    "Veri2=pd.read_csv(\"Datasets/Bert_Tr_Sonuc2.csv\")\n",
    "Veri2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fb0664d",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj= SentimentIntensityAnalyzer()\n",
    "Veri1=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','Tür','Yoğunluk','Tweet_Clear','Tweet_Eng','Tweet_Ayrı_Eng'])\n",
    "sentiment_result=[]\n",
    "for text in tqdm(Veri1[\"Tweet_Ayrı_Eng\"]):\n",
    "    result = sid_obj.polarity_scores(text)\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([Veri1,sentiment_result],axis=1)\n",
    "tweets.columns = tweets.columns.map({'pos':'Pozitif','neg':'Negatif','neu':'Nötr','compound':'Yoğunluk'})\n",
    "tweets.to_csv(\"Datasets/Vader_Tr_Sonuc.csv\",index=False)\n",
    "\n",
    "Veri2=pd.read_csv(\"Datasets/Vader_Tr_Sonuc.csv\")\n",
    "Veri2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c71d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veri3=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','Tür','Yoğunluk','Tweet_Clear'])\n",
    "Veri3[\"Tür\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b5abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Veri3[\"Tür\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in Veri3[\"Tweet_Clear\"])\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910afa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "max_len= 40\n",
    "\n",
    "def tokenize_pad_sequences(text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    # Text tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return X, tokenizer\n",
    "\n",
    "print('Before Tokenization & Padding \\n', Veri3['Tweet_Clear'][0])\n",
    "X, tokenizer = tokenize_pad_sequences(Veri3['Tweet_Clear'])\n",
    "print('After Tokenization & Padding \\n', X[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.document_count\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 16),\n",
    "    tf.keras.layers.LSTM(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fa027",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(Veri3['Tür'])\n",
    "train_data, test_data, train_label, test_label = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "print('Train Set ->', train_data.shape, train_label.shape)\n",
    "print('Test Set ->', test_data.shape, test_label.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca577c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
