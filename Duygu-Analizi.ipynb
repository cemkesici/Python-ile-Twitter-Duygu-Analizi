{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47d3bac8",
   "metadata": {},
   "source": [
    "## İmport İşlemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install tensorflow\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git\n",
    "%pip install transformers datasets\n",
    "%pip install torch\n",
    "%pip install tensorflow\n",
    "%pip install pyautogui\n",
    "%pip install snowballstemmer\n",
    "%pip install vaderSentiment\n",
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import pyautogui\n",
    "import tweepy\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "from snowballstemmer import TurkishStemmer as snowTurkish\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as st\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream ,OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import pad_sequences\n",
    "from trnlp import TrnlpWord\n",
    "from trnlp import *\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer, \n",
    "    TextClassificationPipeline, \n",
    "    pipeline, \n",
    "    AutoModelWithLMHead,\n",
    "    AutoModel,\n",
    "    ElectraTokenizer, \n",
    "    TFElectraForMultipleChoice,\n",
    "    TFElectraForSequenceClassification,\n",
    "    BertTokenizer\n",
    "    )\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from torch.utils.data import TensorDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9efd3cab",
   "metadata": {},
   "source": [
    "## Twitter Veri Çekme ve Temizleme Sınıfı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    \n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"Bağlantı Yapıldı!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler Kayıt Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "            \n",
    "            DataSet['Kullanıcı_Adı'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Degistirilmis'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Ceviri']=[tweet.full_text for tweet in tweets]            \n",
    "            #DataSet['Kullanıcı_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            DataSet.to_csv('Datasets/tw.csv',index=False)\n",
    "            print (\"Veriler Kayıt Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,arananTweet):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulanıyor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=arananTweet,result_type=\"new\", lang='tr',tweet_mode=\"extended\")\n",
    "            results=[]\n",
    "            for item in cursor.items(150):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulandı!\")\n",
    "            DataSet = getTweets.toDataFrame(results)            \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            Dataset=pd.read_csv('Datasets/tw.csv',encoding=\"utf-8\")                   \n",
    "            getTweets.TweetClear(DataSet)#Tweet Temizlemek için\n",
    "            getTweets.SentenceNormalization(DataSet)#Türkçe hatalı yazılmış kelimeleri düzeltmek için\n",
    "            getTweets.TweetTranslate(DataSet)#Tweetleri İngilizceye çevirmek için\n",
    "            getTweets.EngWordSeparation(DataSet)#İngilizce kelime Stopwords \n",
    "            getTweets.TrWordSeparation(DataSet)#Türkçe kelime Stopwords\n",
    "            getTweets.ToxicSentence(DataSet)#İngilizce Argo cümle bulmak için             \n",
    "            getTweets.TrStemmed(DataSet)#Türkçe Kelime köklerini almak için\n",
    "            getTweets.EngStemmed(DataSet)#İngilizce Kelime köklerini almak için\n",
    "            getTweets.TextblobSentiment(DataSet)#Textblob Duygu Analizi (Kelime pozitif negatif olarak sorguluyor)           \n",
    "            DataSet.to_csv('Datasets/tw.csv',index=False)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Duygu Analizi Yapılıyor...\")            \n",
    "            data[\"TextBlobTür\"] = data[\"Tweet_Ayrı_Eng\"].apply(getTweets.getPolarity)\n",
    "            data[\"TextBlobYoğunluk\"] = data[\"Tweet_Ayrı_Eng\"].apply(getTweets.getSubjectivity)\n",
    "            data.to_csv('Datasets/textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi Yapıldı!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            data[\"Tweet_Clear\"]=data[\"Tweet\"]\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|$[a-zA-ZğüşöçıİĞÜŞÖÇ]+|@[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ_]+|[,.^_$*%-;鶯!?’\"“”:=|…#]+|\\n+|amp[\\s]+|gt;')\n",
    "            for index in range(len(data[\"Tweet_Clear\"])):        \n",
    "                data[\"Tweet_Clear\"][index]=pattern.sub(' ', data[\"Tweet_Clear\"][index])    \n",
    "                data[\"Tweet_Clear\"][index]=data[\"Tweet_Clear\"][index].lower()              \n",
    "            data[\"Tweet_Clear\"]=data[\"Tweet_Clear\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))  \n",
    "                              \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Çeviriliyor...\")\n",
    "            data[\"Tweet_Eng\"]=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Eng\"])):\n",
    "                Ceviri_Blob=TextBlob(data[\"Tweet_Eng\"][index])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                data[\"Tweet_Eng\"][index]=Eng_Blob  \n",
    "                data[\"Tweet_Eng\"][index]=data[\"Tweet_Eng\"][index].lower()\n",
    "            print (\"Tweetler Çevirildi!\")                      \n",
    "            return data      \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Cümle Normalizasyonu Yapılıyor...\")\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "            for index in range(len(data[\"Tweet_Clear\"])):                \n",
    "                data[\"Tweet_Clear\"][index]= normalizer.normalize(data[\"Tweet_Clear\"][index].lower())\n",
    "            print(\"Cümle Normalizasyonu Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TrWordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapılıyor...\")\n",
    "            data[\"Tweet_Ayrı_TR\"]=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Ayrı_TR\"])):\n",
    "                data[\"Tweet_Ayrı_TR\"][index] = simple_token(data[\"Tweet_Ayrı_TR\"][index], sw=stopwords)\n",
    "            data[\"Tweet_Ayrı_TR\"] = data[\"Tweet_Ayrı_TR\"].str.join(\" \")\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))  \n",
    "                  \n",
    "    def EngWordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapılıyor...\")\n",
    "            additional  = ['sq','gt','mk','aq','qq','amk']\n",
    "            stop = st.words('english')\n",
    "            \n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Eng\"].str.split()           \n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].apply(lambda x: [item for item in x if item not in additional])\n",
    "            data[\"Tweet_Ayrı_Eng\"] = data[\"Tweet_Ayrı_Eng\"].str.join(\" \")\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TrStemmed(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            data[\"TR_Kök\"] = data[\"Tweet_Ayrı_TR\"]\n",
    "            # Kök Bulma\n",
    "            stemmer = snowTurkish()\n",
    "\n",
    "            stemmed_lists = []\n",
    "            for index in tqdm(data[\"TR_Kök\"].index):\n",
    "                mini_l = []\n",
    "                for text in data.loc[index][\"TR_Kök\"].split(\" \"):\n",
    "                    mini_l.append(stemmer.stemWord(text))\n",
    "\n",
    "                big_text=\" \"\n",
    "                for char in mini_l:\n",
    "                    big_text = big_text + \" \" + char\n",
    "                stemmed_lists.append(big_text)\n",
    "    \n",
    "            DataSet['TR_Kök'] = stemmed_lists\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def EngStemmed(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            data[\"Eng_Kök\"] = data[\"Tweet_Ayrı_Eng\"]\n",
    "            # Kök Bulma\n",
    "            stemmer = PorterStemmer()\n",
    "\n",
    "            stemmed_lists = []\n",
    "            for index in tqdm(data[\"Eng_Kök\"].index):\n",
    "                mini_l = []\n",
    "                for text in data.loc[index][\"Eng_Kök\"].split(\" \"):\n",
    "                    mini_l.append(stemmer.stem(text))\n",
    "\n",
    "                big_text=\" \"\n",
    "                for char in mini_l:\n",
    "                    big_text = big_text + \" \" + char\n",
    "                stemmed_lists.append(big_text)\n",
    "    \n",
    "            DataSet['Eng_Kök'] = stemmed_lists\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def ToxicSentence(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Toxic Cümle Analizi Yapılıyor...\")\n",
    "            model_path =\"martin-ha/toxic-comment-model\"\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "            data[\"Tweet_Toxic\"]=data[\"Tweet_Eng\"]\n",
    "            temp=data[\"Tweet_Clear\"]\n",
    "            for index in range(len(data[\"Tweet_Toxic\"])):\n",
    "                sonuc=pipeline(str(data[\"Tweet_Toxic\"][index]))\n",
    "                if sonuc[0]['label']==\"toxic\":\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic\"\n",
    "                else:\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic değil\"    \n",
    "            print(\"Toxic Cümle Analizi Yapıldı!\")\n",
    "            return data  \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"%75 - %100\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"%50 - %75\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"%25 - %50\"\n",
    "            else:\n",
    "                return \"0 - 25%\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"Nötr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2da544f2",
   "metadata": {},
   "source": [
    "## Class'a Kullanıcı Tarafından Veri Gönderdirilen Bölüm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "while True:\n",
    "    aramaTuru=pyautogui.confirm('Ne Tür Bir Arama Yapacaksınız?',title='Arama Türü', buttons=['Hastag','Kelime','Kullanıcı'])\n",
    "    if aramaTuru==\"Hastag\":\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"#\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kelime':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kullanıcı':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"@\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        secim=pyautogui.confirm(text='Hatalı Seçim Yaptınız Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"Kapat\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e5908b6",
   "metadata": {},
   "source": [
    "## OLUŞTURDUĞUMUZ TWİTTER VERİSETİ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('Datasets/textblob_eng_duygu.csv')\n",
    "Data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc420cb",
   "metadata": {},
   "source": [
    "## BERT MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2b05257",
   "metadata": {},
   "source": [
    "### bert-base-turkish-sentiment-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertData=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','TextBlobTür','TextBlobYoğunluk','Tweet_Clear','Tweet_Eng'])\n",
    "# Eğitilmiş Ağırlıkların Yüklenmesi\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"savasy/bert-base-turkish-sentiment-cased\")\n",
    "\n",
    "sentiment_analysis_pipeline= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "sentiment_result=[]\n",
    "for text in tqdm(BertData[\"Tweet_Clear\"]):\n",
    "    result = sentiment_analysis_pipeline(text)[0]\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([BertData,sentiment_result],axis=1)\n",
    "\n",
    "tweets.to_csv(\"Datasets/Bert_Tr_Sonuc.csv\",index=False)\n",
    "\n",
    "BertData=pd.read_csv(\"Datasets/Bert_Tr_Sonuc.csv\")\n",
    "BertData.rename(columns={'label':'BertTür','score':'BertYoğunluk'}, inplace=True)\n",
    "BertData[\"BertTür\"]=BertData[\"BertTür\"].map({'positive':'Pozitif','negative':'Negatif'})\n",
    "BertData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = BertData[\"BertTür\"].map({\"Pozitif\":'Pozitif',\"Negatif\":'Negatif'}).value_counts().index.values\n",
    "sizes = BertData[\"BertTür\"].value_counts().values\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90,explode=None)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(BertData[\"BertTür\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11df1585",
   "metadata": {},
   "source": [
    "### bert-base-turkish-128k-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac221ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert128Data=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','TextBlobTür','TextBlobYoğunluk','Tweet_Clear','Tweet_Eng'])\n",
    "# Eğitilmiş Ağırlıkların Yüklenmesi\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-uncased\", do_lower_case=True)\n",
    "\n",
    "sentiment_analysis_pipeline= pipeline(\"sentiment-analysis\", tokenizer=tokenizer, model=model)\n",
    "sentiment_result=[]\n",
    "for text in tqdm(Bert128Data[\"Tweet_Clear\"]):\n",
    "    result = sentiment_analysis_pipeline(text)[0]\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([Bert128Data,sentiment_result],axis=1)\n",
    "\n",
    "tweets.to_csv(\"Datasets/Bert128_Tr_Sonuc.csv\",index=False)\n",
    "Bert128Data=pd.read_csv(\"Datasets/Bert128_Tr_Sonuc.csv\")\n",
    "Bert128Data.rename(columns={'label':'Bert128Tür','score':'Bert128Yoğunluk'}, inplace=True)\n",
    "Bert128Data[\"Bert128Tür\"]=Bert128Data[\"Bert128Tür\"].map({'LABEL_0':'Pozitif','LABEL_1':'Negatif'})\n",
    "Bert128Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f620dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Bert128Data[\"Bert128Tür\"].map({\"Pozitif\":'Pozitif',\"Negatif\":'Negatif'}).value_counts().index.values\n",
    "sizes = Bert128Data[\"Bert128Tür\"].value_counts().values\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90,explode=None)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(Bert128Data[\"Bert128Tür\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eda2c7e0",
   "metadata": {},
   "source": [
    "### paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertMiniData=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','TextBlobTür','TextBlobYoğunluk','Tweet_Clear','Tweet_Eng'])\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "sentiment_result=[]\n",
    "# Tokenize sentences\n",
    "for text in BertMiniData[\"Tweet_Eng\"]:\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "sentiment_result = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "\n",
    "tweets = pd.concat([BertMiniData,sentiment_result],axis=1)\n",
    "tweets.to_csv(\"Datasets/BertMini_Sonuc.csv\",index=False)\n",
    "BertMiniData=pd.read_csv(\"Datasets/BertMini_Sonuc.csv\")\n",
    "BertMiniData.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17cea40a",
   "metadata": {},
   "source": [
    "## ELECTRA (BERT MODEL TABANLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91311fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ElectraData=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','TextBlobTür','TextBlobYoğunluk','Tweet_Clear','Tweet_Eng'])\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n",
    "model = TFElectraForSequenceClassification.from_pretrained(\"bhadresh-savani/electra-base-emotion\")\n",
    "\n",
    "sentiment_result=[]\n",
    "for text in tqdm(ElectraData[\"Tweet_Eng\"]):\n",
    "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "    result=model.config.id2label[predicted_class_id]\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([ElectraData,sentiment_result],axis=1)\n",
    "tweets.to_csv(\"Datasets/Electra_Eng_Sonuc.csv\",index=False)\n",
    "\n",
    "ElectraData=pd.read_csv(\"Datasets/Electra_Eng_Sonuc.csv\")\n",
    "ElectraData.rename(columns={\"0\":\"ElectraTür\"}, inplace=True)\n",
    "ElectraData[\"ElectraTür\"]=ElectraData[\"ElectraTür\"].map({\"joy\":'Mutlu',\"anger\":'Kızgın',\"sadness\":\"Üzgün\",\"fear\":\"Korku\"})\n",
    "ElectraData.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ElectraData[\"ElectraTür\"].map({\"Mutlu\":'Mutlu',\"Kızgın\":'Kızgın',\"Üzgün\":\"Üzgün\",\"Korku\":\"Korku\"}).value_counts().index.values\n",
    "sizes = ElectraData[\"ElectraTür\"].value_counts().values\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90,explode=None)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(ElectraData[\"ElectraTür\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fb0664d",
   "metadata": {},
   "source": [
    "## Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj= SentimentIntensityAnalyzer()\n",
    "VaderData=pd.read_csv('Datasets/textblob_eng_duygu.csv',usecols=['Tweet','TextBlobTür','TextBlobYoğunluk','Tweet_Clear','Tweet_Eng','Tweet_Ayrı_Eng'])\n",
    "sentiment_result=[]\n",
    "for text in tqdm(VaderData[\"Tweet_Ayrı_Eng\"]):\n",
    "    result = sid_obj.polarity_scores(text)\n",
    "    sentiment_result.append(result)\n",
    "        \n",
    "sentiment_result = pd.DataFrame(sentiment_result)\n",
    "tweets = pd.concat([VaderData,sentiment_result],axis=1)\n",
    "tweets.to_csv(\"Datasets/Vader_Tr_Sonuc.csv\",index=False)\n",
    "VaderData=pd.read_csv(\"Datasets/Vader_Tr_Sonuc.csv\")\n",
    "VaderData.rename(columns={\"neg\":\"VaderNegatif\",'neu':'VaderNötr','pos':'VaderPozitif','compound':'VaderYoğunluk'}, inplace=True)\n",
    "VaderData.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf624f2",
   "metadata": {},
   "source": [
    "## Model Karşılaştırılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24523c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "tweets = pd.concat([ElectraData,VaderData,BertData,Bert128Data],axis=1)\n",
    "tweets.to_csv(\"Datasets/Duygu_Sonuc.csv\",index=False)\n",
    "temp=pd.read_csv('Datasets/Duygu_Sonuc.csv',usecols=['Tweet','Tweet_Clear','Tweet_Eng','TextBlobTür','ElectraTür','VaderPozitif','VaderNegatif','VaderNötr','BertTür','Bert128Tür'])\n",
    "temp.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f387f8e5",
   "metadata": {},
   "source": [
    "## Son Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a708723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp=pd.read_csv(\"Test/apple-twitter-sentiment-texts.csv\")\n",
    "def TweetClear(DataSet):\n",
    "     try:\n",
    "         data=DataSet\n",
    "         print (\"Tweetler Temizleniyor...\")\n",
    "         data[\"Text_Clear\"]=data[\"text\"]\n",
    "         pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|$[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|@[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ_]+|[,.^_$*%-;鶯!?’\"“”:=|…#]+|\\n+|amp[\\s]+|gt;')\n",
    "         for index in range(len(data[\"Text_Clear\"])):        \n",
    "             data[\"Text_Clear\"][index]=pattern.sub(' ', data[\"Text_Clear\"][index])    \n",
    "             data[\"Text_Clear\"][index]=data[\"Text_Clear\"][index].lower()              \n",
    "         data[\"Text_Clear\"]=data[\"Text_Clear\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "         print (\"Tweetler Temizlendi!\")\n",
    "         return data\n",
    "     except Exception as err:\n",
    "         print('Error: {}'.format(err))  \n",
    "\n",
    "def WordSeparation(DataSet):\n",
    "    try:\n",
    "        data=DataSet\n",
    "        print(\"İngilizce Kelime Ayrıştırma Yapılıyor...\")\n",
    "        additional  = ['sq','gt','mk','aq','qq','amk','rt']\n",
    "        stop = st.words('english')\n",
    "        \n",
    "        data[\"Stopword\"] = data[\"Text_Clear\"].str.split()           \n",
    "        data[\"Stopword\"] = data[\"Stopword\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "        data[\"Stopword\"] = data[\"Stopword\"].apply(lambda x: [item for item in x if item not in additional])\n",
    "        data[\"Stopword\"] = data[\"Stopword\"].str.join(' ')\n",
    "        print(\"İngilizce Kelime Ayrıştırma Yapıldı!\")\n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print('Error: {}'.format(err))\n",
    "  \n",
    "def Stemmed(DataSet):\n",
    "    try:\n",
    "        data=DataSet\n",
    "        data[\"Stemmed\"] = data[\"Stopword\"]\n",
    "        # Kök Bulma\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_lists = []\n",
    "        for index in tqdm(data[\"Stemmed\"].index):\n",
    "            mini_l = []\n",
    "            for text in data.loc[index][\"Stemmed\"].split(\" \"):\n",
    "                mini_l.append(stemmer.stem(text))\n",
    "            big_text=\" \"\n",
    "            for char in mini_l:\n",
    "                big_text = big_text + \" \" + char\n",
    "            stemmed_lists.append(big_text)\n",
    "\n",
    "        DataSet['Stemmed'] = stemmed_lists\n",
    "        return data\n",
    "    except Exception as err:\n",
    "        print('Error: {}'.format(err))\n",
    "        \n",
    "TweetClear(temp)\n",
    "WordSeparation(temp)\n",
    "Stemmed(temp)\n",
    "temp.to_csv(\"Test/testData.csv\",index=False)\n",
    "TestData=pd.read_csv(\"Test/testData.csv\",encoding=\"Utf-8\")\n",
    "TestData.head()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "447adddd",
   "metadata": {},
   "source": [
    "### Tenserflow.Keras Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "673d05f1",
   "metadata": {},
   "source": [
    "Model için ön yukarıda yapıldı işlemler yapıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in TestData[\"Text_Clear\"])\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910afa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "max_len= 40\n",
    "\n",
    "def tokenize_pad_sequences(text):\n",
    "    '''\n",
    "    This function tokenize the input text into sequnences of intergers and then\n",
    "    pad each sequence to the same length\n",
    "    '''\n",
    "    # Text tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    # return sequences\n",
    "    return X, tokenizer\n",
    "\n",
    "print('Before Tokenization & Padding \\n',TestData[\"Text_Clear\"][0])\n",
    "X, tokenizer = tokenize_pad_sequences(TestData[\"Text_Clear\"])\n",
    "print('After Tokenization & Padding \\n', X[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.document_count\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 32),\n",
    "    tf.keras.layers.LSTM(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"relu\")\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395fa027",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(TestData[\"sentiment\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print('Train Set ->', X_train.shape, y_train.shape)\n",
    "print('Test Set ->', X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef161bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "history = model.fit(X_train, y_train, epochs=4, batch_size=64,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a61e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"acc\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f859e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=550, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b729d4",
   "metadata": {},
   "source": [
    "### Sklearn RandomForest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1a45f3",
   "metadata": {},
   "source": [
    "Model için ön yukarıda yapıldı işlemler yapıldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer( max_features=7500, min_df=1, max_df=250, stop_words=st.words('english'))\n",
    "proccessed_features =vectorizer.fit_transform(TestData[\"Text_Clear\"]).toarray()\n",
    "labels = pd.get_dummies(TestData[\"sentiment\"])\n",
    "X_train, X_test, y_train, y_test =train_test_split(proccessed_features, labels, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier=RandomForestClassifier(n_estimators=550, random_state=42)\n",
    "text_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a363dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test,predictions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2adcd2f5",
   "metadata": {},
   "source": [
    "### Bert Model Optimize Edilmiş"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173bb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BertOpData=pd.read_csv('Test/testData.csv')\n",
    "vectorizer = TfidfVectorizer( max_features=7500, min_df=1, max_df=250, stop_words=st.words('english'))\n",
    "proccessed_features =vectorizer.fit_transform(TestData[\"Text_Clear\"]).toarray()\n",
    "labels = pd.get_dummies(TestData[\"sentiment\"])\n",
    "X_train, X_test, y_train, y_test =train_test_split(proccessed_features, labels, test_size=0.3,random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
