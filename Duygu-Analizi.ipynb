{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install poyraz\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git\n",
    "%pip install transformers datasets\n",
    "%pip install torch\n",
    "%pip install tensorflow\n",
    "%pip install pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import logging\n",
    "import pyautogui\n",
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as st\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "from trnlp import TrnlpWord\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "from trnlp import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"Bağlantı Yapıldı!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler Kayıt Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "            \n",
    "            DataSet['Kullanıcı_Adı'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Degistirilmis'] = [tweet.full_text for tweet in tweets]\n",
    "            #DataSet['Tweet_Ceviri']=[tweet.full_text for tweet in tweets]            \n",
    "            #DataSet['Kullanıcı_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            DataSet.to_csv('tw.csv',index=False)\n",
    "            print (\"Veriler Kayıt Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,arananTweet):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulanıyor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=arananTweet,result_type=\"new\", lang='tr',tweet_mode=\"extended\")\n",
    "            results=[]\n",
    "            for item in cursor.items(20):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulandı!\")\n",
    "            DataSet = getTweets.toDataFrame(results)            \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            Dataset=pd.read_csv('tw.csv',encoding=\"utf-8\")                   \n",
    "            getTweets.TweetClear(DataSet)\n",
    "            getTweets.SentenceNormalization(DataSet)\n",
    "            getTweets.TweetTranslate(DataSet)\n",
    "            getTweets.WordSeparation(DataSet) \n",
    "            getTweets.ToxicSentence(DataSet) \n",
    "            getTweets.TrWordSeparation(DataSet)\n",
    "            getTweets.TextblobSentiment(DataSet)           \n",
    "            DataSet.to_csv('tw.csv',index=False)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Duygu Analizi Yapılıyor...\")            \n",
    "            data[\"Tür\"] = data[\"Tweet_Ayrı\"].apply(getTweets.getPolarity)\n",
    "            data[\"Yoğunluk\"] = data[\"Tweet_Ayrı\"].apply(getTweets.getSubjectivity)\n",
    "            data.to_csv('textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi Yapıldı!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def TrWordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapılıyor...\")\n",
    "            data[\"Tweet_Ayrı_TR\"]=data[\"Tweet_Degistirilmis\"]\n",
    "            for index in range(len(data[\"Tweet_Ayrı_TR\"])):\n",
    "                data[\"Tweet_Ayrı_TR\"][index] = simple_token(data[\"Tweet_Ayrı_TR\"][index], sw=stopwords)\n",
    "            print(\"Türkçe Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            data[\"Tweet_Degistirilmis\"]=data[\"Tweet\"]\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|$[a-zA-ZğüşöçıİĞÜŞÖÇ]+|@[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ_]+|[,.^_$*%-;鶯!?’\"“”:=|…#]+|\\n+|RT[\\s]+|amp[\\s]+|gt;')\n",
    "            for index in range(len(data[\"Tweet_Degistirilmis\"])):        \n",
    "                data[\"Tweet_Degistirilmis\"][index]=pattern.sub('', data[\"Tweet_Degistirilmis\"][index])    \n",
    "                data[\"Tweet_Degistirilmis\"][index]=data[\"Tweet_Degistirilmis\"][index].lower()              \n",
    "            data[\"Tweet_Degistirilmis\"]=data[\"Tweet_Degistirilmis\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))  \n",
    "                              \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print (\"Tweetler Çeviriliyor...\")\n",
    "            data[\"Tweet_Ceviri\"]=data[\"Tweet_Degistirilmis\"]\n",
    "            for index in range(len(data[\"Tweet_Ceviri\"])):\n",
    "                Ceviri_Blob=TextBlob(data[\"Tweet_Ceviri\"][index])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                data[\"Tweet_Ceviri\"][index]=Eng_Blob  \n",
    "                data[\"Tweet_Ceviri\"][index]=data[\"Tweet_Ceviri\"][index].lower()\n",
    "            print (\"Tweetler Çevirildi!\")                      \n",
    "            return data      \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Cümle Normalizasyonu Yapılıyor...\")\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "            for index in range(len(data[\"Tweet_Degistirilmis\"])):                \n",
    "                data[\"Tweet_Degistirilmis\"][index]= normalizer.normalize(data[\"Tweet_Degistirilmis\"][index].lower())\n",
    "            print(\"Cümle Normalizasyonu Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def WordSeparation(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapılıyor...\")\n",
    "            additional  = ['sq','gt']\n",
    "            stop = set().union(st.words('english'),additional)\n",
    "            \n",
    "            data[\"Tweet_Ayrı\"] = data[\"Tweet_Ceviri\"].str.split()\n",
    "            data[\"Tweet_Ayrı\"] = data[\"Tweet_Ayrı\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            data[\"Tweet_Ayrı\"] = data[\"Tweet_Ayrı\"].str.join(\" \")\n",
    "            print(\"İngilizce Kelime Ayrıştırma Yapıldı!\")\n",
    "            return data\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def ToxicSentence(DataSet):\n",
    "        try:\n",
    "            data=DataSet\n",
    "            print(\"Toxic Cümle Analizi Yapılıyor...\")\n",
    "            model_path =\"martin-ha/toxic-comment-model\"\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "            data[\"Tweet_Toxic\"]=data[\"Tweet_Ceviri\"]\n",
    "            for index in range(len(data[\"Tweet_Toxic\"])):\n",
    "                sonuc=pipeline(str(data[\"Tweet_Toxic\"][index]))\n",
    "                if sonuc[0]['label']==\"toxic\":\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic\"\n",
    "                else:\n",
    "                    data[\"Tweet_Toxic\"][index]=\"Toxic değil\"    \n",
    "            print(\"Toxic Cümle Analizi Yapıldı!\")\n",
    "            return data  \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"%75 - %100\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"%50 - %75\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"%25 - %50\"\n",
    "            else:\n",
    "                return \"0 - 25%\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"Nötr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "while True:\n",
    "    #aramaTuru=int(pyautogui.prompt(text=\"Ne Tür Bir Arama Yapacaksınız? \\n 1=Hastag \\n 2=Kelime Bazlı Arama \\n 3=Kullanıcı\", title='Arama Türü' , default='1'))\n",
    "    aramaTuru=pyautogui.confirm('Ne Tür Bir Arama Yapacaksınız?',title='Arama Türü', buttons=['Hastag','Kelime','Kullanıcı'])\n",
    "    if aramaTuru==\"Hastag\":\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"#\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kelime':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(arananTweet=temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    elif aramaTuru=='Kullanıcı':\n",
    "        arananKelime=pyautogui.prompt(text=\"Aranacak Kelimeyi Girin:\", title='Aranacak Kelime' , default='python')\n",
    "        temp=(\"@\"+arananKelime+\" -filter:retweets\")\n",
    "        tw.searchTweet(temp)\n",
    "        secim=pyautogui.confirm(text='Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        secim=pyautogui.confirm(text='Hatalı Seçim Yaptınız Tekrar Denemek İster Misiniz?', title='Hata', buttons=[\"Tekrar\",\"İptal\"])\n",
    "        if secim==\"Tekrar\":\n",
    "            continue\n",
    "        else:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veri=pd.read_csv('textblob_eng_duygu.csv')\n",
    "Veri.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Veri.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82299eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eksik Cümle\n",
    "unmasker=pipeline('fill-mask', model='xlm-roberta-base')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
