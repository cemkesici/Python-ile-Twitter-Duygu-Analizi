{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in d:\\programdata\\anaconda3\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: wordcloud in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in d:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: seaborn in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\n",
      "Requirement already satisfied: pandas>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement poyraz (from versions: none)\n",
      "ERROR: No matching distribution found for poyraz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (3.4.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.0.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zemberek-python in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.2.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from zemberek-python) (4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from zemberek-python) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trnltk in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/emres/turkish-deasciifier.git\n",
      "  Cloning https://github.com/emres/turkish-deasciifier.git to c:\\users\\cemke\\appdata\\local\\temp\\pip-req-build-ismsiig6\n",
      "  Resolved https://github.com/emres/turkish-deasciifier.git to commit 665154c734b09485c3d11ce0038cd121dd109594\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/emres/turkish-deasciifier.git 'C:\\Users\\cemke\\AppData\\Local\\Temp\\pip-req-build-ismsiig6'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (4.25.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: dill<0.3.7 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.1.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.29.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install poyraz\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git\n",
    "%pip install transformers datasets\n",
    "%pip install torch\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-11 22:55:17,290 - numexpr.utils - INFO\n",
      "Msg: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "\n",
      "2023-01-11 22:55:17,291 - numexpr.utils - INFO\n",
      "Msg: NumExpr defaulting to 8 threads.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cemke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import logging\n",
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "from trnlp import TrnlpWord\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"BaÄŸlantÄ± YapÄ±ldÄ±!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler KayÄ±t Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "\n",
    "            DataSet['KullanÄ±cÄ±_AdÄ±'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Degistirilmis'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Ceviri']=[tweet.text for tweet in tweets]            \n",
    "            #DataSet['KullanÄ±cÄ±_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            print (\"Veriler KayÄ±t Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,aranan):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler SorgulanÄ±yor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=aranan, result_type=\"new\", lang='tr')\n",
    "            results=[]\n",
    "            for item in cursor.items(10):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler SorgulandÄ±!\")\n",
    "            DataSet = getTweets.toDataFrame(results)  \n",
    "            getTweets.TweetClear(DataSet)\n",
    "            getTweets.SentenceNormalization(DataSet)\n",
    "            getTweets.TweetTranslate(DataSet)\n",
    "            getTweets.KelimeAyristirma(DataSet) \n",
    "            DataSet.to_csv('tw.csv',index=False)\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            getTweets.TextblobSentiment(DataSet)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            print (\"Duygu Analizi YapÄ±lÄ±yor...\")\n",
    "            Veri=DataSet\n",
    "            Veri[\"TÃ¼r\"] = Veri[\"Tweet_AyrÄ±\"].apply(getTweets.getPolarity)\n",
    "            Veri[\"YoÄŸunluk\"] = Veri[\"Tweet_AyrÄ±\"].apply(getTweets.getSubjectivity)\n",
    "            Veri.to_csv('textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi YapÄ±ldÄ±!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    \"\"\"def TrSentiment(DataSet):\n",
    "        try:\n",
    "            print(\"TÃ¼rkÃ§e Duygu Analizi YapÄ±lÄ±yor...\")\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                DataSet[\"TR_A\"][index] = simple_token(example, sw=stopwords)\n",
    "            print(\"TÃ¼rkÃ§e Duygu Analizi YapÄ±ldÄ±!\")\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \"\"\"\n",
    "                     \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Ã‡eviriliyor...\")\n",
    "            for yazi in range(len(DataSet[\"Tweet_Degistirilmis\"])):\n",
    "                Ceviri_Blob=TextBlob(DataSet[\"Tweet_Degistirilmis\"][yazi])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=Eng_Blob\n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=DataSet[\"Tweet_Ceviri\"][yazi].lower()\n",
    "            print (\"Tweetler Ã‡evirildi!\")                      \n",
    "            return DataSet        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°ÄÃœÅÃ–Ã‡]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°ÄÃœÅÃ–Ã‡]+|$[a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°ÄÃœÅÃ–Ã‡]+|@[a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°ÄÃœÅÃ–Ã‡_]+|[,.^_$*%-;é¶¯!?â€™\"â€œâ€:=|â€¦#]+|\\n+|RT+|amp')\n",
    "            for i in range(len(DataSet[\"Tweet_Degistirilmis\"])):        \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=pattern.sub(' ', DataSet[\"Tweet_Degistirilmis\"][i])    \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=DataSet[\"Tweet_Degistirilmis\"][i].lower()              \n",
    "            DataSet[\"Tweet_Degistirilmis\"]=DataSet[\"Tweet_Degistirilmis\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "            start = time.time()\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                 DataSet[\"Tweet_Degistirilmis\"][index]= normalizer.normalize(example)\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def KelimeAyristirma(DataSet):\n",
    "        try:\n",
    "            stop = stopwords.words('english')\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_Ceviri\"].str.split()\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_AyrÄ±\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_AyrÄ±\"].str.join(\" \")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def ToxicSentiment(DataSet):\n",
    "        try:\n",
    "            model_path =\"martin-ha/toxic-comment-model\"\n",
    "            tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "            model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "            text=\"bu cÃ¼mle toksik bir cÃ¼mle deÄŸil amÄ±nakoyayÄ±m\"\n",
    "            b_text=TextBlob(text)\n",
    "            e_text=b_text.translate(from_lang='tr', to='en')  \n",
    "            sonuc=pipeline(str(e_text))\n",
    "            if sonuc[0]['label']==\"toxic\":\n",
    "                print (\"Toxic\")\n",
    "            else:\n",
    "                print (\"Toxic deÄŸil\")        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"Kesinlikle\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"YÃ¼ksek Ä°htimal\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"DÃ¼ÅŸÃ¼k Ä°htimal\"\n",
    "            else:\n",
    "                return \"Kesin DeÄŸil\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"NÃ¶tr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaÄŸlantÄ± YapÄ±ldÄ±!\n",
      "Kelimeye Uygun Tweetler SorgulanÄ±yor...\n",
      "Kelimeye Uygun Tweetler SorgulandÄ±!\n",
      "Veriler KayÄ±t Ediliyor...\n",
      "Veriler KayÄ±t Edildi!\n",
      "Tweetler Temizleniyor...\n",
      "Tweetler Temizlendi!\n",
      "2023-01-11 22:56:20,790 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 7.054992198944092\n",
      "\n",
      "Error: too many values to unpack (expected 2)\n",
      "Tweetler Ã‡eviriliyor...\n",
      "Tweetler Ã‡evirildi!\n",
      "Duygu Analizi YapÄ±lÄ±yor...\n",
      "Duygu Analizi YapÄ±ldÄ±!\n"
     ]
    }
   ],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "aranan=input(\"Aranacak Kelimeyi Girin: \")\n",
    "tw.searchTweet(aranan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KullanÄ±cÄ±_AdÄ±</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_AyrÄ±</th>\n",
       "      <th>TÃ¼r</th>\n",
       "      <th>YoÄŸunluk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZÃ¼htÃ¼</td>\n",
       "      <td>2023-01-11 19:56:13+00:00</td>\n",
       "      <td>RT @HaberMujdeli: Meral AkÅŸener yine bir gÃ¼n hayallerini anlatÄ±rken.. \\n\\nDeliriyorsunuz, kuduruyorsunuz ErdoÄŸan gibi olamadÄ±ÄŸÄ±nÄ±z iÃ§in ğŸ˜Œ\\n\\n hâ€¦</td>\n",
       "      <td>meral akÅŸener yine bir gÃ¼n hayallerini anlatÄ±rken   deliriyorsunuz  kuduruyorsunuz erdoÄŸan gibi olamadÄ±ÄŸÄ±nÄ±z iÃ§in   h</td>\n",
       "      <td>meral akÅŸener again one day while telling your dreams, you get crazy, you can not be like erdogan h.</td>\n",
       "      <td>meral akÅŸener one day telling dreams, get crazy, like erdogan h.</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>Kesinlikle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Umut Can</td>\n",
       "      <td>2023-01-11 19:56:12+00:00</td>\n",
       "      <td>Yine gel akÅŸamÃ¼stÃ¼ , gece sabaha varmadan.</td>\n",
       "      <td>yine gel akÅŸamÃ¼stÃ¼   gece sabaha varmadan</td>\n",
       "      <td>come again after the morning in the morning</td>\n",
       "      <td>come morning morning</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>Kesin DeÄŸil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Åeyda</td>\n",
       "      <td>2023-01-11 19:56:12+00:00</td>\n",
       "      <td>RT @OgretmnEtkinlik: ğŸ“¢ @FOXTurkiye\\n Bu akÅŸam yine 100 bin Ã¶ÄŸretmen dedi. GÃ¼ndemde olmaya devam ediyoruz. Cumhuriyetin 100. YÄ±lÄ±na Ã¶zel 100â€¦</td>\n",
       "      <td>bu akÅŸam yine   bin Ã¶ÄŸretmen dedi  gÃ¼ndemde olmaya devam ediyoruz  cumhuriyetin   yÄ±lÄ±na Ã¶zel</td>\n",
       "      <td>we continue to be on the agenda of a thousand teachers again this evening, we continue to be on the agenda.</td>\n",
       "      <td>continue agenda thousand teachers evening, continue agenda.</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>Kesin DeÄŸil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>â„³á¯á‰â„ ğŸ’™</td>\n",
       "      <td>2023-01-11 19:56:12+00:00</td>\n",
       "      <td>RT @arto_sagatelyan: Yine gÃ¶zÃ¼me Ã§arptÄ± \\nNe zaman rastlarsam sinirlerim zÄ±plÄ±yor ğŸ˜ ğŸ˜¡ğŸ¤¬ https://t.co/c5XbM0Ubnx</td>\n",
       "      <td>yine gÃ¶zÃ¼me Ã§arptÄ±  ne zaman rastlarsam sinirlerim zÄ±plÄ±yor</td>\n",
       "      <td>it hit me again, whenever i come across my nerves jump</td>\n",
       "      <td>hit again, whenever come across nerves jump</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>Kesin DeÄŸil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÅÃ¼kru BircanğŸ‡¹ğŸ‡·ğŸ‡µğŸ‡¸</td>\n",
       "      <td>2023-01-11 19:56:12+00:00</td>\n",
       "      <td>RT @demir_erdem: SevdamÄ±z var diyerek yine yollara dÃ¼ÅŸtÃ¼kâ€¦\\n\\nBugÃ¼n,\\n\\nğŸ“KÄ±sÄ±klÄ± \\nğŸ“Salacak\\nğŸ“KÃ¼plÃ¼ce Mahallelerimizde ziyaretlerimizi gerÃ§ekleÅŸtâ€¦</td>\n",
       "      <td>sevdamÄ±z var diyerek yine yollara dÃ¼ÅŸtÃ¼k  bugÃ¼n  kÄ±sÄ±klÄ±  salacak kÃ¼plÃ¼ce mahallelerimizde ziyaretlerimizi gerÃ§ekleÅŸt</td>\n",
       "      <td>we have fell on the roads again by saying that we have love today.</td>\n",
       "      <td>fell roads saying love today.</td>\n",
       "      <td>Pozitif</td>\n",
       "      <td>YÃ¼ksek Ä°htimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      KullanÄ±cÄ±_AdÄ±                Tweet_Tarih  \\\n",
       "0             ZÃ¼htÃ¼  2023-01-11 19:56:13+00:00   \n",
       "1          Umut Can  2023-01-11 19:56:12+00:00   \n",
       "2             Åeyda  2023-01-11 19:56:12+00:00   \n",
       "3            â„³á¯á‰â„ ğŸ’™  2023-01-11 19:56:12+00:00   \n",
       "4  ÅÃ¼kru BircanğŸ‡¹ğŸ‡·ğŸ‡µğŸ‡¸  2023-01-11 19:56:12+00:00   \n",
       "\n",
       "                                                                                                                                                Tweet  \\\n",
       "0    RT @HaberMujdeli: Meral AkÅŸener yine bir gÃ¼n hayallerini anlatÄ±rken.. \\n\\nDeliriyorsunuz, kuduruyorsunuz ErdoÄŸan gibi olamadÄ±ÄŸÄ±nÄ±z iÃ§in ğŸ˜Œ\\n\\n hâ€¦   \n",
       "1                                                                                                          Yine gel akÅŸamÃ¼stÃ¼ , gece sabaha varmadan.   \n",
       "2        RT @OgretmnEtkinlik: ğŸ“¢ @FOXTurkiye\\n Bu akÅŸam yine 100 bin Ã¶ÄŸretmen dedi. GÃ¼ndemde olmaya devam ediyoruz. Cumhuriyetin 100. YÄ±lÄ±na Ã¶zel 100â€¦   \n",
       "3                                       RT @arto_sagatelyan: Yine gÃ¶zÃ¼me Ã§arptÄ± \\nNe zaman rastlarsam sinirlerim zÄ±plÄ±yor ğŸ˜ ğŸ˜¡ğŸ¤¬ https://t.co/c5XbM0Ubnx   \n",
       "4  RT @demir_erdem: SevdamÄ±z var diyerek yine yollara dÃ¼ÅŸtÃ¼kâ€¦\\n\\nBugÃ¼n,\\n\\nğŸ“KÄ±sÄ±klÄ± \\nğŸ“Salacak\\nğŸ“KÃ¼plÃ¼ce Mahallelerimizde ziyaretlerimizi gerÃ§ekleÅŸtâ€¦   \n",
       "\n",
       "                                                                                                           Tweet_Degistirilmis  \\\n",
       "0       meral akÅŸener yine bir gÃ¼n hayallerini anlatÄ±rken   deliriyorsunuz  kuduruyorsunuz erdoÄŸan gibi olamadÄ±ÄŸÄ±nÄ±z iÃ§in   h    \n",
       "1                                                                                   yine gel akÅŸamÃ¼stÃ¼   gece sabaha varmadan    \n",
       "2                              bu akÅŸam yine   bin Ã¶ÄŸretmen dedi  gÃ¼ndemde olmaya devam ediyoruz  cumhuriyetin   yÄ±lÄ±na Ã¶zel     \n",
       "3                                                               yine gÃ¶zÃ¼me Ã§arptÄ±  ne zaman rastlarsam sinirlerim zÄ±plÄ±yor      \n",
       "4       sevdamÄ±z var diyerek yine yollara dÃ¼ÅŸtÃ¼k  bugÃ¼n  kÄ±sÄ±klÄ±  salacak kÃ¼plÃ¼ce mahallelerimizde ziyaretlerimizi gerÃ§ekleÅŸt    \n",
       "\n",
       "                                                                                                  Tweet_Ceviri  \\\n",
       "0         meral akÅŸener again one day while telling your dreams, you get crazy, you can not be like erdogan h.   \n",
       "1                                                                  come again after the morning in the morning   \n",
       "2  we continue to be on the agenda of a thousand teachers again this evening, we continue to be on the agenda.   \n",
       "3                                                       it hit me again, whenever i come across my nerves jump   \n",
       "4                                           we have fell on the roads again by saying that we have love today.   \n",
       "\n",
       "                                                         Tweet_AyrÄ±      TÃ¼r  \\\n",
       "0  meral akÅŸener one day telling dreams, get crazy, like erdogan h.  Negatif   \n",
       "1                                              come morning morning     NÃ¶tr   \n",
       "2       continue agenda thousand teachers evening, continue agenda.     NÃ¶tr   \n",
       "3                       hit again, whenever come across nerves jump     NÃ¶tr   \n",
       "4                                     fell roads saying love today.  Pozitif   \n",
       "\n",
       "         YoÄŸunluk  \n",
       "0      Kesinlikle  \n",
       "1     Kesin DeÄŸil  \n",
       "2     Kesin DeÄŸil  \n",
       "3     Kesin DeÄŸil  \n",
       "4  YÃ¼ksek Ä°htimal  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri=pd.read_csv('textblob_eng_duygu.csv')\n",
    "Veri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a62d44e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic deÄŸil\n"
     ]
    }
   ],
   "source": [
    "#Argo cÃ¼mle\n",
    "model_path =\"martin-ha/toxic-comment-model\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "text=\"bu cÃ¼mle toksik bir cÃ¼mle deÄŸil amÄ±nakoyayÄ±m\"\n",
    "b_text=TextBlob(text)\n",
    "e_text=b_text.translate(from_lang='tr', to='en')  \n",
    "sonuc=pipeline(str(e_text))\n",
    "if sonuc[0]['label']==\"toxic\":\n",
    "    print (\"Toxic\")\n",
    "else:\n",
    "    print (\"Toxic deÄŸil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82299eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3272\\1534099532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Eksik CÃ¼mle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munmasker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fill-mask'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xlm-roberta-base'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pipelines\\text_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mper\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;31m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0m_legacy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"top_k\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1074\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1075\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36mrun_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m         \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1081\u001b[0m         \u001b[0mmodel_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\pipelines\\text_classification.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;34m' dictionnary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             )\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2518\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2519\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2520\u001b[1;33m             \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2521\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2624\u001b[0m             )\n\u001b[0;32m   2625\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2626\u001b[1;33m             return self.encode_plus(\n\u001b[0m\u001b[0;32m   2627\u001b[0m                 \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2628\u001b[0m                 \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2697\u001b[0m         )\n\u001b[0;32m   2698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2699\u001b[1;33m         return self._encode_plus(\n\u001b[0m\u001b[0;32m   2700\u001b[0m             \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2701\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "#Eksik CÃ¼mle\n",
    "unmasker=pipeline('fill-mask', model='xlm-roberta-base')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
