{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in d:\\programdata\\anaconda3\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wordcloud in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: textblob in d:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: pandas>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: scipy>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement poyraz (from versions: none)\n",
      "ERROR: No matching distribution found for poyraz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (3.4.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.0.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: zemberek-python in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.2.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from zemberek-python) (4.8)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from zemberek-python) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trnltk in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/emres/turkish-deasciifier.git\n",
      "  Cloning https://github.com/emres/turkish-deasciifier.git to c:\\users\\cemke\\appdata\\local\\temp\\pip-req-build-4aommr_i\n",
      "  Resolved https://github.com/emres/turkish-deasciifier.git to commit 665154c734b09485c3d11ce0038cd121dd109594\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/emres/turkish-deasciifier.git 'C:\\Users\\cemke\\AppData\\Local\\Temp\\pip-req-build-4aommr_i'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (4.25.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "     -------------------------------------- 452.9/452.9 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: dill<0.3.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-win_amd64.whl (20.3 MB)\n",
      "     ---------------------------------------- 20.3/20.3 MB 4.8 MB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.5/323.5 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.8.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script plasma_store.exe is installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "     -------------------------------------- 162.5/162.5 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     -------------------------------------- 124.6/124.6 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.9/895.9 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     ---------------------------------------- 23.2/23.2 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.8/177.8 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, keras, google-pasta, gast, cachetools, astunparse, absl-py, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.12\n",
      "    Uninstalling protobuf-4.21.12:\n",
      "      Successfully uninstalled protobuf-4.21.12\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.1 flatbuffers-23.1.4 gast-0.4.0 google-auth-2.16.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.11.0 libclang-15.0.6.1 opt-einsum-3.3.0 protobuf-3.19.6 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.29.0 termcolor-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\cemke\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.51.1 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install poyraz\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git\n",
    "%pip install transformers datasets\n",
    "%pip install torch\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cemke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import logging\n",
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "from trnlp import TrnlpWord\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"Bağlantı Yapıldı!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler Kayıt Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "\n",
    "            DataSet['Kullanıcı_Adı'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Degistirilmis'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Ceviri']=[tweet.text for tweet in tweets]            \n",
    "            #DataSet['Kullanıcı_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            print (\"Veriler Kayıt Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,aranan):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulanıyor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=aranan, result_type=\"new\", lang='tr')\n",
    "            results=[]\n",
    "            for item in cursor.items(10):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler Sorgulandı!\")\n",
    "            DataSet = getTweets.toDataFrame(results)  \n",
    "            getTweets.TweetClear(DataSet)\n",
    "            getTweets.SentenceNormalization(DataSet)\n",
    "            getTweets.TweetTranslate(DataSet)\n",
    "            getTweets.KelimeAyristirma(DataSet) \n",
    "            DataSet.to_csv('tw.csv',index=False)\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            getTweets.TextblobSentiment(DataSet)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            print (\"Duygu Analizi Yapılıyor...\")\n",
    "            Veri=DataSet\n",
    "            Veri[\"Tür\"] = Veri[\"Tweet_Ayrı\"].apply(getTweets.getPolarity)\n",
    "            Veri[\"Yoğunluk\"] = Veri[\"Tweet_Ayrı\"].apply(getTweets.getSubjectivity)\n",
    "            Veri.to_csv('textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi Yapıldı!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    \"\"\"def TrSentiment(DataSet):\n",
    "        try:\n",
    "            print(\"Türkçe Duygu Analizi Yapılıyor...\")\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                DataSet[\"TR_A\"][index] = simple_token(example, sw=stopwords)\n",
    "            print(\"Türkçe Duygu Analizi Yapıldı!\")\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \"\"\"\n",
    "                     \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Çeviriliyor...\")\n",
    "            for yazi in range(len(DataSet[\"Tweet_Degistirilmis\"])):\n",
    "                Ceviri_Blob=TextBlob(DataSet[\"Tweet_Degistirilmis\"][yazi])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=Eng_Blob\n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=DataSet[\"Tweet_Ceviri\"][yazi].lower()\n",
    "            print (\"Tweetler Çevirildi!\")                      \n",
    "            return DataSet        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            pattern = re.compile('http[s]?://(?:[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F]))+|#[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|$[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ]+|@[a-zA-Z0-9ğüşöçıİĞÜŞÖÇ_]+|[,.^_$*%-;鶯!?’\"“”:=|…#]+|\\n+|RT+|amp')\n",
    "            for i in range(len(DataSet[\"Tweet_Degistirilmis\"])):        \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=pattern.sub(' ', DataSet[\"Tweet_Degistirilmis\"][i])    \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=DataSet[\"Tweet_Degistirilmis\"][i].lower()              \n",
    "            DataSet[\"Tweet_Degistirilmis\"]=DataSet[\"Tweet_Degistirilmis\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "            start = time.time()\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                 DataSet[\"Tweet_Degistirilmis\"][index]= normalizer.normalize(example)\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def KelimeAyristirma(DataSet):\n",
    "        try:\n",
    "            stop = stopwords.words('english')\n",
    "            DataSet[\"Tweet_Ayrı\"] = DataSet[\"Tweet_Ceviri\"].str.split()\n",
    "            DataSet[\"Tweet_Ayrı\"] = DataSet[\"Tweet_Ayrı\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            DataSet[\"Tweet_Ayrı\"] = DataSet[\"Tweet_Ayrı\"].str.join(\" \")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"Kesinlikle\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"Yüksek İhtimal\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"Düşük İhtimal\"\n",
    "            else:\n",
    "                return \"Kesin Değil\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"Nötr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bağlantı Yapıldı!\n",
      "Kelimeye Uygun Tweetler Sorgulanıyor...\n",
      "Kelimeye Uygun Tweetler Sorgulandı!\n",
      "Veriler Kayıt Ediliyor...\n",
      "Veriler Kayıt Edildi!\n",
      "Tweetler Temizleniyor...\n",
      "Tweetler Temizlendi!\n",
      "2023-01-11 04:27:39,814 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 9.935980796813965\n",
      "\n",
      "Error: too many values to unpack (expected 2)\n",
      "Tweetler Çeviriliyor...\n",
      "Tweetler Çevirildi!\n",
      "Duygu Analizi Yapılıyor...\n",
      "Duygu Analizi Yapıldı!\n"
     ]
    }
   ],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "aranan=input(\"Aranacak Kelimeyi Girin: \")\n",
    "tw.searchTweet(aranan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kullanıcı_Adı</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_Ayrı</th>\n",
       "      <th>Tür</th>\n",
       "      <th>Yoğunluk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alka.fatih</td>\n",
       "      <td>2023-01-11 01:27:14+00:00</td>\n",
       "      <td>@ankara_kusu Bu işe yaptırım uygulamak için daha ne olması gerekiyor???</td>\n",
       "      <td>bu işe yaptırım uygulamak için daha ne olması gerekiyor</td>\n",
       "      <td>what more needs to be to impose sanctions on this job</td>\n",
       "      <td>needs impose sanctions job</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H. Serkan Oğuz</td>\n",
       "      <td>2023-01-11 01:27:12+00:00</td>\n",
       "      <td>RT @serdartunccc: İlk günkü inanç ve azimle, hep birlikte Genel Başkanımız @meral_aksener öncülüğünde İYİ Parti iktidarına ilerliyoruz. Ank…</td>\n",
       "      <td>ilk günkü inanç ve azimle  hep birlikte genel başkanımız   öncülüğünde iyi parti iktidarına ilerliyoruz  ank</td>\n",
       "      <td>with the belief and determination of the first day, we are moving to the good party power under the leadership of our president.</td>\n",
       "      <td>belief determination first day, moving good party power leadership president.</td>\n",
       "      <td>Pozitif</td>\n",
       "      <td>Düşük İhtimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ataman Atölye</td>\n",
       "      <td>2023-01-11 01:27:07+00:00</td>\n",
       "      <td>RT @Facemas23085764: @ankara_kusu Nasıl bulmadı.\\nAnında karşılık buldu .Hemde 1 haftada 1M yapıştırdılar üstüne.! https://t.co/4UDNAsKYOu</td>\n",
       "      <td>nasıl bulmadı  anında karşılık buldu  hemde   haftada  m yapıştırdılar üstüne</td>\n",
       "      <td>how did he not find it instantly found and glued it per week</td>\n",
       "      <td>find instantly found glued per week</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Yüksek İhtimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>balkiz</td>\n",
       "      <td>2023-01-11 01:26:48+00:00</td>\n",
       "      <td>dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken… https://t.co/36Hze9I145</td>\n",
       "      <td>dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken</td>\n",
       "      <td>my mother had a book set on the internet for my brother and i said it to my father's work address.</td>\n",
       "      <td>mother book set internet brother said father's work address.</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murat Çelik</td>\n",
       "      <td>2023-01-11 01:26:32+00:00</td>\n",
       "      <td>RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/dte4jOeqvj</td>\n",
       "      <td>ankara     ocak idareciler günü kutlama programı</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Kullanıcı_Adı                Tweet_Tarih  \\\n",
       "0      alka.fatih  2023-01-11 01:27:14+00:00   \n",
       "1  H. Serkan Oğuz  2023-01-11 01:27:12+00:00   \n",
       "2   Ataman Atölye  2023-01-11 01:27:07+00:00   \n",
       "3          balkiz  2023-01-11 01:26:48+00:00   \n",
       "4     Murat Çelik  2023-01-11 01:26:32+00:00   \n",
       "\n",
       "                                                                                                                                          Tweet  \\\n",
       "0                                                                       @ankara_kusu Bu işe yaptırım uygulamak için daha ne olması gerekiyor???   \n",
       "1  RT @serdartunccc: İlk günkü inanç ve azimle, hep birlikte Genel Başkanımız @meral_aksener öncülüğünde İYİ Parti iktidarına ilerliyoruz. Ank…   \n",
       "2    RT @Facemas23085764: @ankara_kusu Nasıl bulmadı.\\nAnında karşılık buldu .Hemde 1 haftada 1M yapıştırdılar üstüne.! https://t.co/4UDNAsKYOu   \n",
       "3  dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken… https://t.co/36Hze9I145   \n",
       "4                                   RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/dte4jOeqvj   \n",
       "\n",
       "                                                                                                      Tweet_Degistirilmis  \\\n",
       "0                                                                bu işe yaptırım uygulamak için daha ne olması gerekiyor    \n",
       "1           ilk günkü inanç ve azimle  hep birlikte genel başkanımız   öncülüğünde iyi parti iktidarına ilerliyoruz  ank    \n",
       "2                                        nasıl bulmadı  anında karşılık buldu  hemde   haftada  m yapıştırdılar üstüne      \n",
       "3  dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken      \n",
       "4                                                                      ankara     ocak idareciler günü kutlama programı     \n",
       "\n",
       "                                                                                                                       Tweet_Ceviri  \\\n",
       "0                                                                             what more needs to be to impose sanctions on this job   \n",
       "1  with the belief and determination of the first day, we are moving to the good party power under the leadership of our president.   \n",
       "2                                                                      how did he not find it instantly found and glued it per week   \n",
       "3                                my mother had a book set on the internet for my brother and i said it to my father's work address.   \n",
       "4                                                                             ankara january administrators day celebration program   \n",
       "\n",
       "                                                                      Tweet_Ayrı  \\\n",
       "0                                                     needs impose sanctions job   \n",
       "1  belief determination first day, moving good party power leadership president.   \n",
       "2                                            find instantly found glued per week   \n",
       "3                   mother book set internet brother said father's work address.   \n",
       "4                          ankara january administrators day celebration program   \n",
       "\n",
       "       Tür        Yoğunluk  \n",
       "0     Nötr     Kesin Değil  \n",
       "1  Pozitif   Düşük İhtimal  \n",
       "2     Nötr  Yüksek İhtimal  \n",
       "3     Nötr     Kesin Değil  \n",
       "4     Nötr     Kesin Değil  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri=pd.read_csv('textblob_eng_duygu.csv')\n",
    "Veri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "608e49f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kullanıcı_Adı</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_Ayrı</th>\n",
       "      <th>Tür</th>\n",
       "      <th>Yoğunluk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Murat Çelik</td>\n",
       "      <td>2023-01-11 01:26:25+00:00</td>\n",
       "      <td>RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/LNhJ7P3Iqk</td>\n",
       "      <td>ankara     ocak idareciler günü kutlama programı</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sedat Tuna</td>\n",
       "      <td>2023-01-11 01:26:10+00:00</td>\n",
       "      <td>RT @oakarsucelik: HALK TV Ankara Temsilciliği görevimden istifamın gerekçesi👇🏽\\nDört aydır birlikte çalıştığım ekip arkadaşlarıma ve değerli…</td>\n",
       "      <td>halk tv ankara temsilciliği görevimden istifamın gerekçesi dört aydır birlikte çalıştığım ekip arkadaşlarıma ve değerli</td>\n",
       "      <td>the justification for my resignation from my duty as halk tv ankara representative for my teammates and valuable</td>\n",
       "      <td>justification resignation duty halk tv ankara representative teammates valuable</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tc.İbrahim Çevlik</td>\n",
       "      <td>2023-01-11 01:26:00+00:00</td>\n",
       "      <td>RT @medyaankaracom: İYİ Parti Ankara İl Başkan Adayı olan Serdar Tunç'a destek yağıyor @serdartunccc #iyiparti #Ankara  https://t.co/SorBgN…</td>\n",
       "      <td>iyi parti ankara il başkan adayı olan serdar tunç a destek yağıyor</td>\n",
       "      <td>good party ankara provincial president candidate serdar tunç'a supports support</td>\n",
       "      <td>good party ankara provincial president candidate serdar tunç'a supports support</td>\n",
       "      <td>Pozitif</td>\n",
       "      <td>Yüksek İhtimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dengiz</td>\n",
       "      <td>2023-01-11 01:25:58+00:00</td>\n",
       "      <td>RT @BalabanUtku: Bugün Xavier Üniversitesi'nde sosyoloji doçenti olarak göreve başladım. \\n\\nBu vesileyle beş sene önce devleti arkasına alar…</td>\n",
       "      <td>bugün xavier üniversitesi nde sosyoloji doçenti olarak göreve başladım   bu vesileyle beş sene önce devleti arkasına alar</td>\n",
       "      <td>today i started to work as an associate professor at xavier university.</td>\n",
       "      <td>today started work associate professor xavier university.</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ataman Atölye</td>\n",
       "      <td>2023-01-11 01:25:39+00:00</td>\n",
       "      <td>RT @A_Khr2021: @ankara_kusu 0.69’luk ev kredisi söylemleri SAHADA KARŞILIK BULDU!🥹 https://t.co/xIHDaqf9Lr</td>\n",
       "      <td>luk ev kredisi söylemleri sahada karşilik buldu</td>\n",
       "      <td>luk home loan discourses found in the field</td>\n",
       "      <td>luk home loan discourses found field</td>\n",
       "      <td>Nötr</td>\n",
       "      <td>Kesin Değil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Kullanıcı_Adı                Tweet_Tarih  \\\n",
       "5        Murat Çelik  2023-01-11 01:26:25+00:00   \n",
       "6         Sedat Tuna  2023-01-11 01:26:10+00:00   \n",
       "7  Tc.İbrahim Çevlik  2023-01-11 01:26:00+00:00   \n",
       "8             dengiz  2023-01-11 01:25:58+00:00   \n",
       "9      Ataman Atölye  2023-01-11 01:25:39+00:00   \n",
       "\n",
       "                                                                                                                                            Tweet  \\\n",
       "5                                     RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/LNhJ7P3Iqk   \n",
       "6   RT @oakarsucelik: HALK TV Ankara Temsilciliği görevimden istifamın gerekçesi👇🏽\\nDört aydır birlikte çalıştığım ekip arkadaşlarıma ve değerli…   \n",
       "7    RT @medyaankaracom: İYİ Parti Ankara İl Başkan Adayı olan Serdar Tunç'a destek yağıyor @serdartunccc #iyiparti #Ankara  https://t.co/SorBgN…   \n",
       "8  RT @BalabanUtku: Bugün Xavier Üniversitesi'nde sosyoloji doçenti olarak göreve başladım. \\n\\nBu vesileyle beş sene önce devleti arkasına alar…   \n",
       "9                                      RT @A_Khr2021: @ankara_kusu 0.69’luk ev kredisi söylemleri SAHADA KARŞILIK BULDU!🥹 https://t.co/xIHDaqf9Lr   \n",
       "\n",
       "                                                                                                               Tweet_Degistirilmis  \\\n",
       "5                                                                               ankara     ocak idareciler günü kutlama programı     \n",
       "6         halk tv ankara temsilciliği görevimden istifamın gerekçesi dört aydır birlikte çalıştığım ekip arkadaşlarıma ve değerli    \n",
       "7                                                     iyi parti ankara il başkan adayı olan serdar tunç a destek yağıyor             \n",
       "8       bugün xavier üniversitesi nde sosyoloji doçenti olarak göreve başladım   bu vesileyle beş sene önce devleti arkasına alar    \n",
       "9                                                                               luk ev kredisi söylemleri sahada karşilik buldu      \n",
       "\n",
       "                                                                                                       Tweet_Ceviri  \\\n",
       "5                                                             ankara january administrators day celebration program   \n",
       "6  the justification for my resignation from my duty as halk tv ankara representative for my teammates and valuable   \n",
       "7                                   good party ankara provincial president candidate serdar tunç'a supports support   \n",
       "8                                           today i started to work as an associate professor at xavier university.   \n",
       "9                                                                       luk home loan discourses found in the field   \n",
       "\n",
       "                                                                        Tweet_Ayrı  \\\n",
       "5                            ankara january administrators day celebration program   \n",
       "6  justification resignation duty halk tv ankara representative teammates valuable   \n",
       "7  good party ankara provincial president candidate serdar tunç'a supports support   \n",
       "8                        today started work associate professor xavier university.   \n",
       "9                                             luk home loan discourses found field   \n",
       "\n",
       "       Tür        Yoğunluk  \n",
       "5     Nötr     Kesin Değil  \n",
       "6     Nötr     Kesin Değil  \n",
       "7  Pozitif  Yüksek İhtimal  \n",
       "8     Nötr     Kesin Değil  \n",
       "9     Nötr     Kesin Değil  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e2020640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kullanıcı_Adı</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_Ayrı</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alka.fatih</td>\n",
       "      <td>2023-01-11 01:27:14+00:00</td>\n",
       "      <td>@ankara_kusu Bu işe yaptırım uygulamak için daha ne olması gerekiyor???</td>\n",
       "      <td>bu işe yaptırım uygulamak için daha ne olması gerekiyor</td>\n",
       "      <td>what more needs to be to impose sanctions on this job</td>\n",
       "      <td>needs impose sanctions job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H. Serkan Oğuz</td>\n",
       "      <td>2023-01-11 01:27:12+00:00</td>\n",
       "      <td>RT @serdartunccc: İlk günkü inanç ve azimle, hep birlikte Genel Başkanımız @meral_aksener öncülüğünde İYİ Parti iktidarına ilerliyoruz. Ank…</td>\n",
       "      <td>ilk günkü inanç ve azimle  hep birlikte genel başkanımız   öncülüğünde iyi parti iktidarına ilerliyoruz  ank</td>\n",
       "      <td>with the belief and determination of the first day, we are moving to the good party power under the leadership of our president.</td>\n",
       "      <td>belief determination first day, moving good party power leadership president.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ataman Atölye</td>\n",
       "      <td>2023-01-11 01:27:07+00:00</td>\n",
       "      <td>RT @Facemas23085764: @ankara_kusu Nasıl bulmadı.\\nAnında karşılık buldu .Hemde 1 haftada 1M yapıştırdılar üstüne.! https://t.co/4UDNAsKYOu</td>\n",
       "      <td>nasıl bulmadı  anında karşılık buldu  hemde   haftada  m yapıştırdılar üstüne</td>\n",
       "      <td>how did he not find it instantly found and glued it per week</td>\n",
       "      <td>find instantly found glued per week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>balkiz</td>\n",
       "      <td>2023-01-11 01:26:48+00:00</td>\n",
       "      <td>dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken… https://t.co/36Hze9I145</td>\n",
       "      <td>dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken</td>\n",
       "      <td>my mother had a book set on the internet for my brother and i said it to my father's work address.</td>\n",
       "      <td>mother book set internet brother said father's work address.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murat Çelik</td>\n",
       "      <td>2023-01-11 01:26:32+00:00</td>\n",
       "      <td>RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/dte4jOeqvj</td>\n",
       "      <td>ankara     ocak idareciler günü kutlama programı</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "      <td>ankara january administrators day celebration program</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Kullanıcı_Adı                Tweet_Tarih  \\\n",
       "0      alka.fatih  2023-01-11 01:27:14+00:00   \n",
       "1  H. Serkan Oğuz  2023-01-11 01:27:12+00:00   \n",
       "2   Ataman Atölye  2023-01-11 01:27:07+00:00   \n",
       "3          balkiz  2023-01-11 01:26:48+00:00   \n",
       "4     Murat Çelik  2023-01-11 01:26:32+00:00   \n",
       "\n",
       "                                                                                                                                          Tweet  \\\n",
       "0                                                                       @ankara_kusu Bu işe yaptırım uygulamak için daha ne olması gerekiyor???   \n",
       "1  RT @serdartunccc: İlk günkü inanç ve azimle, hep birlikte Genel Başkanımız @meral_aksener öncülüğünde İYİ Parti iktidarına ilerliyoruz. Ank…   \n",
       "2    RT @Facemas23085764: @ankara_kusu Nasıl bulmadı.\\nAnında karşılık buldu .Hemde 1 haftada 1M yapıştırdılar üstüne.! https://t.co/4UDNAsKYOu   \n",
       "3  dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken… https://t.co/36Hze9I145   \n",
       "4                                   RT @suleymansoylu: #CanlıYayın📡 |📍Ankara | 10 Ocak İdareciler Günü Kutlama Programı https://t.co/dte4jOeqvj   \n",
       "\n",
       "                                                                                                      Tweet_Degistirilmis  \\\n",
       "0                                                                bu işe yaptırım uygulamak için daha ne olması gerekiyor    \n",
       "1           ilk günkü inanç ve azimle  hep birlikte genel başkanımız   öncülüğünde iyi parti iktidarına ilerliyoruz  ank    \n",
       "2                                        nasıl bulmadı  anında karşılık buldu  hemde   haftada  m yapıştırdılar üstüne      \n",
       "3  dun annem kardesim icin internetten kitap seti aldirtti ve babamin is adresine soyledim bugun tarot destesi alirken      \n",
       "4                                                                      ankara     ocak idareciler günü kutlama programı     \n",
       "\n",
       "                                                                                                                       Tweet_Ceviri  \\\n",
       "0                                                                             what more needs to be to impose sanctions on this job   \n",
       "1  with the belief and determination of the first day, we are moving to the good party power under the leadership of our president.   \n",
       "2                                                                      how did he not find it instantly found and glued it per week   \n",
       "3                                my mother had a book set on the internet for my brother and i said it to my father's work address.   \n",
       "4                                                                             ankara january administrators day celebration program   \n",
       "\n",
       "                                                                      Tweet_Ayrı  \n",
       "0                                                     needs impose sanctions job  \n",
       "1  belief determination first day, moving good party power leadership president.  \n",
       "2                                            find instantly found glued per week  \n",
       "3                   mother book set internet brother said father's work address.  \n",
       "4                          ankara january administrators day celebration program  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri1=pd.read_csv('tw.csv')\n",
    "Veri1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a62d44e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12476\\3154322927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"martin-ha/toxic-comment-model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTextClassificationPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Gözlerin güzel\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "#Argo cümle\n",
    "model_path =\"martin-ha/toxic-comment-model\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "pipeline=TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "text=\"Gözlerin güzel\"\n",
    "sonuc=pipeline(str(text))\n",
    "if sonuc[0][\"label\"]==\"toxic\":\n",
    "    print (\"Toxic\")\n",
    "else:\n",
    "    print (\"Toxic değil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82299eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eksik Cümle\n",
    "unmasker=pipeline('fill-mask', model='xlm-roberta-base')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
