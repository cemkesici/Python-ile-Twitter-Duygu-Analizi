{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50cb347",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in d:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tweepy in d:\\programdata\\anaconda3\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.28.1)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: wordcloud in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: matplotlib in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in d:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: textblob in d:\\programdata\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: joblib in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn in d:\\programdata\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\n",
      "Requirement already satisfied: pandas>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement poyraz (from versions: none)\n",
      "ERROR: No matching distribution found for poyraz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (3.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.11)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.1.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.10.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\cemke\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "%pip install tweepy\n",
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install textblob\n",
    "%pip install seaborn\n",
    "%pip install poyraz\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install zemberek-python\n",
    "%pip install trnltk\n",
    "%pip install git+https://github.com/emres/turkish-deasciifier.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73c9b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cemke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*-coding:utf-8-*-\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "import logging\n",
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import string\n",
    "import pandas as pd\n",
    "import ast\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import Stream\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "from trnlp import TrnlpWord\n",
    "from turkish.deasciifier import Deasciifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c724d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTweets:\n",
    "    def __init__(self,ckey,csecret,atoken,asecret):\n",
    "        self.api=None\n",
    "        self.consumer_key = ckey\n",
    "        self.consumer_secret = csecret\n",
    "        self.access_token = atoken\n",
    "        self.access_token_secret = asecret\n",
    "        \n",
    "    def setAuth(self):\n",
    "        try:\n",
    "            self.auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "            self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "            self.api = tweepy.API(self.auth)            \n",
    "            print (\"BaÄŸlantÄ± YapÄ±ldÄ±!\")                           \n",
    "        except tweepy.TweepError as err:\n",
    "            print('Error: {}'.format(err))   \n",
    "    \n",
    "    def toDataFrame(tweets):\n",
    "        try:\n",
    "            print (\"Veriler KayÄ±t Ediliyor...\")\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "            DataSet = pd.DataFrame()\n",
    "\n",
    "            DataSet['KullanÄ±cÄ±_AdÄ±'] = [tweet.user.name for tweet in tweets]\n",
    "            DataSet['Tweet_Tarih'] = [tweet.created_at for tweet in tweets]   \n",
    "            #DataSet['Tweet_Id'] = [tweet.id for tweet in tweets]\n",
    "            DataSet['Tweet'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Degistirilmis'] = [tweet.text for tweet in tweets]\n",
    "            DataSet['Tweet_Ceviri']=[tweet.text for tweet in tweets]            \n",
    "            #DataSet['KullanÄ±cÄ±_Id'] = [tweet.user.id for tweet in tweets]  \n",
    "                \n",
    "            #DataSet['Dil'] = [tweet.lang for tweet in tweets]\n",
    "            print (\"Veriler KayÄ±t Edildi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "    \n",
    "    def searchTweet(self,aranan):\n",
    "        self.setAuth()\n",
    "        try:                      \n",
    "            print (\"Kelimeye Uygun Tweetler SorgulanÄ±yor...\")\n",
    "            cursor = tweepy.Cursor(self.api.search_tweets, q=aranan, result_type=\"new\", lang='tr')\n",
    "            results=[]\n",
    "            for item in cursor.items(10):\n",
    "                results.append(item)\n",
    "            print (\"Kelimeye Uygun Tweetler SorgulandÄ±!\")\n",
    "            DataSet = getTweets.toDataFrame(results)  \n",
    "            getTweets.TweetClear(DataSet)\n",
    "            getTweets.SentenceNormalization(DataSet)\n",
    "            getTweets.TweetTranslate(DataSet)\n",
    "            getTweets.KelimeAyristirma(DataSet) \n",
    "            DataSet.to_csv('tw.csv',index=False)\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        finally:\n",
    "            getTweets.TextblobSentiment(DataSet)\n",
    "\n",
    "    def TextblobSentiment(DataSet):\n",
    "        try:\n",
    "            print (\"Duygu Analizi YapÄ±lÄ±yor...\")\n",
    "            Veri=DataSet\n",
    "            Veri[\"TÃ¼r\"] = Veri[\"Tweet_AyrÄ±\"].apply(getTweets.getPolarity)\n",
    "            Veri[\"YoÄŸunluk\"] = Veri[\"Tweet_AyrÄ±\"].apply(getTweets.getSubjectivity)\n",
    "            Veri.to_csv('textblob_eng_duygu.csv',index=False)\n",
    "            print(\"Duygu Analizi YapÄ±ldÄ±!\")                        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \n",
    "            \n",
    "    \"\"\"def TrSentiment(DataSet):\n",
    "        try:\n",
    "            print(\"TÃ¼rkÃ§e Duygu Analizi YapÄ±lÄ±yor...\")\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                DataSet[\"TR_A\"][index] = simple_token(example, sw=stopwords)\n",
    "            print(\"TÃ¼rkÃ§e Duygu Analizi YapÄ±ldÄ±!\")\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err)) \"\"\"\n",
    "                     \n",
    "    def TweetTranslate(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Ã‡eviriliyor...\")\n",
    "            for yazi in range(len(DataSet[\"Tweet_Degistirilmis\"])):\n",
    "                Ceviri_Blob=TextBlob(DataSet[\"Tweet_Degistirilmis\"][yazi])\n",
    "                Eng_Blob=Ceviri_Blob.translate(from_lang='tr', to='en')  \n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=Eng_Blob\n",
    "                DataSet[\"Tweet_Ceviri\"][yazi]=DataSet[\"Tweet_Ceviri\"][yazi].lower()\n",
    "            print (\"Tweetler Ã‡evirildi!\")                      \n",
    "            return DataSet        \n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def TweetClear(DataSet):\n",
    "        try:\n",
    "            print (\"Tweetler Temizleniyor...\")\n",
    "            pattern = re.compile('http[s]?://(?:[A-Za-z0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|#[A-Za-z0-9]+|$[A-Za-z0-9]+|@[A-Za-z0-9_]+|[,.^_$*%-;é¶¯!?â€™\"â€œâ€:=|â€¦]+|\\n+|RT+|&amp')\n",
    "            for i in range(len(DataSet[\"Tweet_Degistirilmis\"])):        \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=pattern.sub('', DataSet[\"Tweet_Degistirilmis\"][i])    \n",
    "                DataSet[\"Tweet_Degistirilmis\"][i]=DataSet[\"Tweet_Degistirilmis\"][i].lower()              \n",
    "            DataSet[\"Tweet_Degistirilmis\"]=DataSet[\"Tweet_Degistirilmis\"].str.replace(\"[^\\w\\s]\",\"\")\n",
    "            print (\"Tweetler Temizlendi!\")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def SentenceNormalization(DataSet):\n",
    "        try:\n",
    "            morphology = TurkishMorphology.create_with_defaults()\n",
    "            normalizer = TurkishSentenceNormalizer(morphology)\n",
    "\n",
    "            start = time.time()\n",
    "            for example,index in DataSet[\"Tweet_Degistirilmis\"]:\n",
    "                 DataSet[\"Tweet_Degistirilmis\"][index]= normalizer.normalize(example)\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def KelimeAyristirma(DataSet):\n",
    "        try:\n",
    "            stop = stopwords.words('english')\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_Ceviri\"].str.split()\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_AyrÄ±\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "            DataSet[\"Tweet_AyrÄ±\"] = DataSet[\"Tweet_AyrÄ±\"].str.join(\" \")\n",
    "            return DataSet\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "        \n",
    "    def getSubjectivity(DataSet):\n",
    "        try: \n",
    "            sub=TextBlob(DataSet).sentiment.subjectivity               \n",
    "            if sub >=0.75:\n",
    "                return \"Kesinlikle\"\n",
    "            elif sub >=0.5 and sub <0.75:\n",
    "                return \"YÃ¼ksek Ä°htimal\"\n",
    "            elif sub >=0.25 and sub <0.50:\n",
    "                return \"DÃ¼ÅŸÃ¼k Ä°htimal\"\n",
    "            else:\n",
    "                return \"Kesin DeÄŸil\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))\n",
    "            \n",
    "    def getPolarity(DataSet):\n",
    "        try:\n",
    "            pol=TextBlob(DataSet).sentiment.polarity\n",
    "            if pol>0.1:\n",
    "                return \"Pozitif\"\n",
    "            elif pol<-0.1:\n",
    "                return \"Negatif\"\n",
    "            else:\n",
    "                return \"NÃ¶tr\"\n",
    "        except Exception as err:\n",
    "            print('Error: {}'.format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc4db959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaÄŸlantÄ± YapÄ±ldÄ±!\n",
      "Kelimeye Uygun Tweetler SorgulanÄ±yor...\n",
      "Kelimeye Uygun Tweetler SorgulandÄ±!\n",
      "Veriler KayÄ±t Ediliyor...\n",
      "Veriler KayÄ±t Edildi!\n",
      "Tweetler Temizleniyor...\n",
      "Tweetler Temizlendi!\n",
      "2023-01-10 03:16:22,120 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 8.251937627792358\n",
      "\n",
      "Error: too many values to unpack (expected 2)\n",
      "Tweetler Ã‡eviriliyor...\n",
      "Tweetler Ã‡evirildi!\n",
      "Duygu Analizi YapÄ±lÄ±yor...\n",
      "Duygu Analizi YapÄ±ldÄ±!\n"
     ]
    }
   ],
   "source": [
    "tw = getTweets( \n",
    "ckey = 'o7qidRODRWm5u07PtAWrCpSqf',\n",
    "csecret = '1Yn60OYMJl5kRCSM0XQ4Lskxh6hjM4VMz51OdsS6dM9DQuQ4MV',\n",
    "atoken = '1315792402234441730-Dt7AydgVGYI5VN9RRQfnR8Endh0WAc',\n",
    "asecret = 'jhfiDfB53Z27EqHfvfKbNdo0w2SWLqyyoAkILqgSoaGMK'\n",
    ")\n",
    "aranan=input(\"Aranacak Kelimeyi Girin: \")\n",
    "tw.searchTweet(aranan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e018df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KullanÄ±cÄ±_AdÄ±</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_AyrÄ±</th>\n",
       "      <th>TÃ¼r</th>\n",
       "      <th>YoÄŸunluk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Janel ğŸ“–</td>\n",
       "      <td>2023-01-10 00:16:10+00:00</td>\n",
       "      <td>269343888 defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebilâ€¦ https://t.co/sJmsMsJtED</td>\n",
       "      <td>defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebil</td>\n",
       "      <td>i was going to laurel.</td>\n",
       "      <td>going laurel.</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>Kesin DeÄŸil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEHMET AKIF DEMIR</td>\n",
       "      <td>2023-01-10 00:15:22+00:00</td>\n",
       "      <td>@bbcturkce Ozon delindi diyorlar tamam! Ozon yama tuttu diyorlar tamam!Kara delik keÅŸfettik diyorlar tamam! UzayÄ±nâ€¦ https://t.co/LOJh18iSsY</td>\n",
       "      <td>ozon delindi diyorlar tamam ozon yama tuttu diyorlar tamamkara delik keÅŸfettik diyorlar tamam uzayÄ±n</td>\n",
       "      <td>they say ozone is punctured, they say ozone kept patch.</td>\n",
       "      <td>say ozone punctured, say ozone kept patch.</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>Kesin DeÄŸil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hakan Kanat Ø®Ø§Ù‚Ø§Ù† Ù‚Ù†Ø§Ø¯</td>\n",
       "      <td>2023-01-10 00:14:29+00:00</td>\n",
       "      <td>RT @baarrss__: Bu iÅŸler bÃ¶yledir; Halep demezseniz Hatayâ€™Ä± tartÄ±ÅŸmaya aÃ§arlar, Musul-KerkÃ¼k demezseniz DiyarbakÄ±râ€™a gÃ¶z koyarlar, Selanik dâ€¦</td>\n",
       "      <td>bu iÅŸler bÃ¶yledir halep demezseniz hatayÄ± tartÄ±ÅŸmaya aÃ§arlar musulkerkÃ¼k demezseniz diyarbakÄ±ra gÃ¶z koyarlar selanik d</td>\n",
       "      <td>if you do not say aleppo, they will open the mistake if you don't say mosulkerkuk, they put an eye on diyarbakÄ±r.</td>\n",
       "      <td>say aleppo, open mistake say mosulkerkuk, put eye diyarbakÄ±r.</td>\n",
       "      <td>NÃ¶tr</td>\n",
       "      <td>YÃ¼ksek Ä°htimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ÅAH-I MERDAN</td>\n",
       "      <td>2023-01-10 00:14:24+00:00</td>\n",
       "      <td>RT @KarasoyMurad: KÄ±ymetli Dostlar,\\nyurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden KHKâ€™lÄ±nÄ±n, faÅŸist iktidarÄ±n kara propaganda aâ€¦</td>\n",
       "      <td>kÄ±ymetli dostlaryurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden khklÄ±nÄ±n faÅŸist iktidarÄ±n kara propaganda a</td>\n",
       "      <td>dear friends who died after the fire opened while trying to go to the fascist power of the black propaganda a</td>\n",
       "      <td>dear friends died fire opened trying go fascist power black propaganda</td>\n",
       "      <td>Negatif</td>\n",
       "      <td>DÃ¼ÅŸÃ¼k Ä°htimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>2023-01-10 00:13:56+00:00</td>\n",
       "      <td>Seni Ã§ok seviyorum n'e'fesim. ğŸ–¤\\nBiliyorum hep benimlesin..\\nGÃ¼lÃ¼ÅŸÃ¼n, bakÄ±ÅŸlarÄ±n, kokun, nefesin..\\nBu acÄ± hiÃ§ geÃ§mediâ€¦ https://t.co/pIR2WJVxb5</td>\n",
       "      <td>seni Ã§ok seviyorum nefesim biliyorum hep benimlesingÃ¼lÃ¼ÅŸÃ¼n bakÄ±ÅŸlarÄ±n kokun nefesinbu acÄ± hiÃ§ geÃ§medi</td>\n",
       "      <td>i love you so much, i know my breath, your gaze is always your breath, your breath has never passed</td>\n",
       "      <td>love much, know breath, gaze always breath, breath never passed</td>\n",
       "      <td>Pozitif</td>\n",
       "      <td>DÃ¼ÅŸÃ¼k Ä°htimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            KullanÄ±cÄ±_AdÄ±                Tweet_Tarih  \\\n",
       "0                 Janel ğŸ“–  2023-01-10 00:16:10+00:00   \n",
       "1       MEHMET AKIF DEMIR  2023-01-10 00:15:22+00:00   \n",
       "2  Hakan Kanat Ø®Ø§Ù‚Ø§Ù† Ù‚Ù†Ø§Ø¯  2023-01-10 00:14:29+00:00   \n",
       "3            ÅAH-I MERDAN  2023-01-10 00:14:24+00:00   \n",
       "4                     ...  2023-01-10 00:13:56+00:00   \n",
       "\n",
       "                                                                                                                                             Tweet  \\\n",
       "0     269343888 defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebilâ€¦ https://t.co/sJmsMsJtED   \n",
       "1      @bbcturkce Ozon delindi diyorlar tamam! Ozon yama tuttu diyorlar tamam!Kara delik keÅŸfettik diyorlar tamam! UzayÄ±nâ€¦ https://t.co/LOJh18iSsY   \n",
       "2     RT @baarrss__: Bu iÅŸler bÃ¶yledir; Halep demezseniz Hatayâ€™Ä± tartÄ±ÅŸmaya aÃ§arlar, Musul-KerkÃ¼k demezseniz DiyarbakÄ±râ€™a gÃ¶z koyarlar, Selanik dâ€¦   \n",
       "3    RT @KarasoyMurad: KÄ±ymetli Dostlar,\\nyurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden KHKâ€™lÄ±nÄ±n, faÅŸist iktidarÄ±n kara propaganda aâ€¦   \n",
       "4  Seni Ã§ok seviyorum n'e'fesim. ğŸ–¤\\nBiliyorum hep benimlesin..\\nGÃ¼lÃ¼ÅŸÃ¼n, bakÄ±ÅŸlarÄ±n, kokun, nefesin..\\nBu acÄ± hiÃ§ geÃ§mediâ€¦ https://t.co/pIR2WJVxb5   \n",
       "\n",
       "                                                                                                        Tweet_Degistirilmis  \\\n",
       "0                defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebil    \n",
       "1                     ozon delindi diyorlar tamam ozon yama tuttu diyorlar tamamkara delik keÅŸfettik diyorlar tamam uzayÄ±n    \n",
       "2    bu iÅŸler bÃ¶yledir halep demezseniz hatayÄ± tartÄ±ÅŸmaya aÃ§arlar musulkerkÃ¼k demezseniz diyarbakÄ±ra gÃ¶z koyarlar selanik d   \n",
       "3     kÄ±ymetli dostlaryurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden khklÄ±nÄ±n faÅŸist iktidarÄ±n kara propaganda a   \n",
       "4                    seni Ã§ok seviyorum nefesim biliyorum hep benimlesingÃ¼lÃ¼ÅŸÃ¼n bakÄ±ÅŸlarÄ±n kokun nefesinbu acÄ± hiÃ§ geÃ§medi    \n",
       "\n",
       "                                                                                                        Tweet_Ceviri  \\\n",
       "0                                                                                             i was going to laurel.   \n",
       "1                                                            they say ozone is punctured, they say ozone kept patch.   \n",
       "2  if you do not say aleppo, they will open the mistake if you don't say mosulkerkuk, they put an eye on diyarbakÄ±r.   \n",
       "3      dear friends who died after the fire opened while trying to go to the fascist power of the black propaganda a   \n",
       "4                i love you so much, i know my breath, your gaze is always your breath, your breath has never passed   \n",
       "\n",
       "                                                               Tweet_AyrÄ±  \\\n",
       "0                                                           going laurel.   \n",
       "1                              say ozone punctured, say ozone kept patch.   \n",
       "2           say aleppo, open mistake say mosulkerkuk, put eye diyarbakÄ±r.   \n",
       "3  dear friends died fire opened trying go fascist power black propaganda   \n",
       "4         love much, know breath, gaze always breath, breath never passed   \n",
       "\n",
       "       TÃ¼r        YoÄŸunluk  \n",
       "0     NÃ¶tr     Kesin DeÄŸil  \n",
       "1     NÃ¶tr     Kesin DeÄŸil  \n",
       "2     NÃ¶tr  YÃ¼ksek Ä°htimal  \n",
       "3  Negatif   DÃ¼ÅŸÃ¼k Ä°htimal  \n",
       "4  Pozitif   DÃ¼ÅŸÃ¼k Ä°htimal  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri=pd.read_csv('textblob_eng_duygu.csv')\n",
    "Veri.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2020640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KullanÄ±cÄ±_AdÄ±</th>\n",
       "      <th>Tweet_Tarih</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet_Degistirilmis</th>\n",
       "      <th>Tweet_Ceviri</th>\n",
       "      <th>Tweet_AyrÄ±</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Janel ğŸ“–</td>\n",
       "      <td>2023-01-10 00:16:10+00:00</td>\n",
       "      <td>269343888 defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebilâ€¦ https://t.co/sJmsMsJtED</td>\n",
       "      <td>defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebil</td>\n",
       "      <td>i was going to laurel.</td>\n",
       "      <td>going laurel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEHMET AKIF DEMIR</td>\n",
       "      <td>2023-01-10 00:15:22+00:00</td>\n",
       "      <td>@bbcturkce Ozon delindi diyorlar tamam! Ozon yama tuttu diyorlar tamam!Kara delik keÅŸfettik diyorlar tamam! UzayÄ±nâ€¦ https://t.co/LOJh18iSsY</td>\n",
       "      <td>ozon delindi diyorlar tamam ozon yama tuttu diyorlar tamamkara delik keÅŸfettik diyorlar tamam uzayÄ±n</td>\n",
       "      <td>they say ozone is punctured, they say ozone kept patch.</td>\n",
       "      <td>say ozone punctured, say ozone kept patch.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hakan Kanat Ø®Ø§Ù‚Ø§Ù† Ù‚Ù†Ø§Ø¯</td>\n",
       "      <td>2023-01-10 00:14:29+00:00</td>\n",
       "      <td>RT @baarrss__: Bu iÅŸler bÃ¶yledir; Halep demezseniz Hatayâ€™Ä± tartÄ±ÅŸmaya aÃ§arlar, Musul-KerkÃ¼k demezseniz DiyarbakÄ±râ€™a gÃ¶z koyarlar, Selanik dâ€¦</td>\n",
       "      <td>bu iÅŸler bÃ¶yledir halep demezseniz hatayÄ± tartÄ±ÅŸmaya aÃ§arlar musulkerkÃ¼k demezseniz diyarbakÄ±ra gÃ¶z koyarlar selanik d</td>\n",
       "      <td>if you do not say aleppo, they will open the mistake if you don't say mosulkerkuk, they put an eye on diyarbakÄ±r.</td>\n",
       "      <td>say aleppo, open mistake say mosulkerkuk, put eye diyarbakÄ±r.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ÅAH-I MERDAN</td>\n",
       "      <td>2023-01-10 00:14:24+00:00</td>\n",
       "      <td>RT @KarasoyMurad: KÄ±ymetli Dostlar,\\nyurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden KHKâ€™lÄ±nÄ±n, faÅŸist iktidarÄ±n kara propaganda aâ€¦</td>\n",
       "      <td>kÄ±ymetli dostlaryurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden khklÄ±nÄ±n faÅŸist iktidarÄ±n kara propaganda a</td>\n",
       "      <td>dear friends who died after the fire opened while trying to go to the fascist power of the black propaganda a</td>\n",
       "      <td>dear friends died fire opened trying go fascist power black propaganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>2023-01-10 00:13:56+00:00</td>\n",
       "      <td>Seni Ã§ok seviyorum n'e'fesim. ğŸ–¤\\nBiliyorum hep benimlesin..\\nGÃ¼lÃ¼ÅŸÃ¼n, bakÄ±ÅŸlarÄ±n, kokun, nefesin..\\nBu acÄ± hiÃ§ geÃ§mediâ€¦ https://t.co/pIR2WJVxb5</td>\n",
       "      <td>seni Ã§ok seviyorum nefesim biliyorum hep benimlesingÃ¼lÃ¼ÅŸÃ¼n bakÄ±ÅŸlarÄ±n kokun nefesinbu acÄ± hiÃ§ geÃ§medi</td>\n",
       "      <td>i love you so much, i know my breath, your gaze is always your breath, your breath has never passed</td>\n",
       "      <td>love much, know breath, gaze always breath, breath never passed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            KullanÄ±cÄ±_AdÄ±                Tweet_Tarih  \\\n",
       "0                 Janel ğŸ“–  2023-01-10 00:16:10+00:00   \n",
       "1       MEHMET AKIF DEMIR  2023-01-10 00:15:22+00:00   \n",
       "2  Hakan Kanat Ø®Ø§Ù‚Ø§Ù† Ù‚Ù†Ø§Ø¯  2023-01-10 00:14:29+00:00   \n",
       "3            ÅAH-I MERDAN  2023-01-10 00:14:24+00:00   \n",
       "4                     ...  2023-01-10 00:13:56+00:00   \n",
       "\n",
       "                                                                                                                                             Tweet  \\\n",
       "0     269343888 defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebilâ€¦ https://t.co/sJmsMsJtED   \n",
       "1      @bbcturkce Ozon delindi diyorlar tamam! Ozon yama tuttu diyorlar tamam!Kara delik keÅŸfettik diyorlar tamam! UzayÄ±nâ€¦ https://t.co/LOJh18iSsY   \n",
       "2     RT @baarrss__: Bu iÅŸler bÃ¶yledir; Halep demezseniz Hatayâ€™Ä± tartÄ±ÅŸmaya aÃ§arlar, Musul-KerkÃ¼k demezseniz DiyarbakÄ±râ€™a gÃ¶z koyarlar, Selanik dâ€¦   \n",
       "3    RT @KarasoyMurad: KÄ±ymetli Dostlar,\\nyurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden KHKâ€™lÄ±nÄ±n, faÅŸist iktidarÄ±n kara propaganda aâ€¦   \n",
       "4  Seni Ã§ok seviyorum n'e'fesim. ğŸ–¤\\nBiliyorum hep benimlesin..\\nGÃ¼lÃ¼ÅŸÃ¼n, bakÄ±ÅŸlarÄ±n, kokun, nefesin..\\nBu acÄ± hiÃ§ geÃ§mediâ€¦ https://t.co/pIR2WJVxb5   \n",
       "\n",
       "                                                                                                        Tweet_Degistirilmis  \\\n",
       "0                defneyapraÄŸÄ±acaktÄ±m etyaranacaklar kara sabanacaklar glÃ¼tenecektim gÃ¼cendiriciecektim formÃ¼lleÅŸtirmekebil    \n",
       "1                     ozon delindi diyorlar tamam ozon yama tuttu diyorlar tamamkara delik keÅŸfettik diyorlar tamam uzayÄ±n    \n",
       "2    bu iÅŸler bÃ¶yledir halep demezseniz hatayÄ± tartÄ±ÅŸmaya aÃ§arlar musulkerkÃ¼k demezseniz diyarbakÄ±ra gÃ¶z koyarlar selanik d   \n",
       "3     kÄ±ymetli dostlaryurtdÄ±ÅŸÄ±na Ã§Ä±kmak isterken aÃ§Ä±lan ateÅŸ sonrasÄ± vefat eden khklÄ±nÄ±n faÅŸist iktidarÄ±n kara propaganda a   \n",
       "4                    seni Ã§ok seviyorum nefesim biliyorum hep benimlesingÃ¼lÃ¼ÅŸÃ¼n bakÄ±ÅŸlarÄ±n kokun nefesinbu acÄ± hiÃ§ geÃ§medi    \n",
       "\n",
       "                                                                                                        Tweet_Ceviri  \\\n",
       "0                                                                                             i was going to laurel.   \n",
       "1                                                            they say ozone is punctured, they say ozone kept patch.   \n",
       "2  if you do not say aleppo, they will open the mistake if you don't say mosulkerkuk, they put an eye on diyarbakÄ±r.   \n",
       "3      dear friends who died after the fire opened while trying to go to the fascist power of the black propaganda a   \n",
       "4                i love you so much, i know my breath, your gaze is always your breath, your breath has never passed   \n",
       "\n",
       "                                                               Tweet_AyrÄ±  \n",
       "0                                                           going laurel.  \n",
       "1                              say ozone punctured, say ozone kept patch.  \n",
       "2           say aleppo, open mistake say mosulkerkuk, put eye diyarbakÄ±r.  \n",
       "3  dear friends died fire opened trying go fascist power black propaganda  \n",
       "4         love much, know breath, gaze always breath, breath never passed  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Veri1=pd.read_csv('tw.csv')\n",
    "Veri1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d44e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
